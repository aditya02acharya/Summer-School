{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Iowa Gambling Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are presented with four virtual decks of cards. You are told that each time you choose a card from a deck, you will either win or lose some money. The goal is to win as much money as possible.\n",
    "\n",
    "This is the basis of the Iowa Gambling Task. Some decks will win money in the long term, on average, while some will lose money in the long term, on average.\n",
    "\n",
    "This experiment was introduced by Bechara et al. in 1994, and was as initially used to test to see if certain brain dysfunctions affected the card choice, though we will use it to generally model decision making and learning. \n",
    "\n",
    "This tutorial will teach you to create a model of this task in Python. The learning method we will use is called 'reinforcement learning', a learning method where the system uses rewards to determine the best decision to make, or in this case, the best deck to draw a card from.\n",
    "\n",
    "We will be basing this tutorial on an experiment done by Bull et al., 2015, which conducted a version of the Iowa Gambling task.\n",
    "\n",
    "This is an image of what the Iowa Gambling Task looks like, and how participants interact with it, taken from the paper.\n",
    "\n",
    "As you can see, there are four virtual decks. Deck C has been chosen, and the participant receives $50 from that choice.\n",
    "\n",
    "<img src=\"IGT.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial has been produced using Jupyter Notebook. You will see sections of text, accompanied by sections of code. To run the code, click on it, hold shift, and press enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Decks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's begin writing the code. We will first write the code to set up the decks. The code used in this tutorial has been based on Farris, 2016, though it has been heavily adapted and rewritten in order to suit this tutorial.\n",
    "\n",
    "Each section in the tutorial will begin with a block of code, followed by a line by line explanation of the code. The code most likely won't make much sense before reading the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomised order of the decks:  [3, 1, 0, 2] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Iowa:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.deck_values = [0, 1, 2, 3]\n",
    "\n",
    "        random.shuffle(self.deck_values)  # randomise deck order\n",
    "\n",
    "        self.num_draws = [0, 0, 0, 0]  # set up list for tracking number of draws\n",
    "        self.est_values = [0, 0, 0, 0]  # set up list for tracking estimated values\n",
    "        \n",
    "        print \"Randomised order of the decks: \", self.deck_values, '\\n';\n",
    "\n",
    "experiment_instance = Iowa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This initial function sets up the decks, and randomises the order. A function is some code that performs a certain task. \n",
    "\n",
    "Run the code above a few times and see the results. This is the order of the four decks, generated randomly each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line of code:\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "allows us to use the Python library 'numpy' by calling 'np'. Libraries contain many useful functions. In our case, numpy contains mathematical functions that we will be using.\n",
    "\n",
    "    import random\n",
    "\n",
    "similarly imports the 'random' library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a class is created. Classes hold functions that we will create. We have named our class 'Iowa'.\n",
    "\n",
    "An initial '\\__init\\__' function has been created. This is used to set up the experiment initially.\n",
    "\n",
    "'\\__init\\__' functions are special within Python. When we want to use this class to run an experiment, we need to make an 'instance' of the class. Whenever we make an instance, the '\\__init\\__ function is automatically run.\n",
    "\n",
    "A good analogy would be this:\n",
    "\n",
    "A class contains some sort of structure. A 'dog' class for example, could contain 'legs', 'eyes', etc.\n",
    "\n",
    "An instance of this dog class would a specific creation. You can have as many instances as possible. An example of an instance could be 'James' or 'Dolly', it would be a specific dog.\n",
    "\n",
    "A good guide to classes, instances, and self, by Hamrick, 2011, is linked in the references section at the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.deck_values = [0, 1, 2, 3]\n",
    "\n",
    "'self' refers to this particular instance. It means this particular instance will have this 'deck_values' list.\n",
    "\n",
    "A list is a data structure that holds elements (items), similar to arrays in other languages. This line sets up a list, with elements 0, 1, 2, 3. These are the the deck numbers which we will refer to as the 'base decks'. Like many other languages, Python begins counting at 0.\n",
    "\n",
    "I would highly recommend reading the TutorialPoint introduction to lists, linked in the references section at the bottom for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    random.shuffle(self.deck_values)\n",
    "    \n",
    "This calls the 'shuffle' function from the 'random' library. This shuffles the elements in the list, randomising the order of the decks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    self.num_draws = [0, 0, 0, 0]\n",
    "\n",
    "This line of code sets up a list of four numbers, all set to zero to start. We will use this to keep track of how many times each deck has been drawn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    self.est_values = [0, 0, 0, 0]\n",
    "\n",
    "This sets up another list of four numbers, with all set to zero to start again. We will use this to keep track of what we estimate the return of each deck to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take another look at the code, and run it a few times to make sure you understand what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Calculating the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will now write a function to calculate the reward after drawing from a deck. The code may look a little intimidating but we'll walk through it line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deck 0:\n",
      "Reward:  -35\n",
      "Good: 1 \n",
      "\n",
      "Deck 1:\n",
      "Reward:  50\n",
      "Good: -1 \n",
      "\n",
      "Deck 2:\n",
      "Reward:  -150\n",
      "Good: -1 \n",
      "\n",
      "Deck 3:\n",
      "Reward:  50\n",
      "Good: 1 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Iowa:\n",
    "    \n",
    "    from code import __init__\n",
    "    \n",
    "    def calc_reward(self, deck_to_draw):\n",
    "        \n",
    "        deck_number = self.deck_values[deck_to_draw]  # the deck number to draw from\n",
    "\n",
    "        good = 0  # if the deck is good or bad\n",
    "        \n",
    "        rand_num = random.random()  # random float between 0.0 and 1.0\n",
    "\n",
    "        if deck_number == 0:  # average over 10 draws, -250\n",
    "\n",
    "            good = -1  # bad deck\n",
    "\n",
    "            if rand_num < 0.5:\n",
    "                reward = -150\n",
    "            else:\n",
    "                reward = 100\n",
    "\n",
    "        elif deck_number == 1:  # average over 10 draws, -250\n",
    "\n",
    "            good = -1  # bad deck\n",
    "\n",
    "            if rand_num < 0.1:\n",
    "                reward = -700\n",
    "            else:\n",
    "                reward = 50\n",
    "\n",
    "        elif deck_number == 2:  # average over 10 draws, +200\n",
    "\n",
    "            good = 1  # good deck\n",
    "\n",
    "            if rand_num < 0.5:\n",
    "                reward = -35\n",
    "            else:\n",
    "                reward = 75\n",
    "\n",
    "        elif deck_number == 3:  # average over 10 draws, +250\n",
    "\n",
    "            good = 1  # good deck\n",
    "\n",
    "            if rand_num < 0.1:\n",
    "                reward = -200\n",
    "            else:\n",
    "                reward = 50\n",
    "        \n",
    "        print \"Reward: \", reward\n",
    "        print \"Good:\", good, '\\n';\n",
    "        return reward, good\n",
    "\n",
    "experiment_instance = Iowa()  \n",
    "\n",
    "print 'Deck 0:'\n",
    "experiment_instance.calc_reward(0)\n",
    "\n",
    "print 'Deck 1:'\n",
    "experiment_instance.calc_reward(1)\n",
    "\n",
    "print 'Deck 2:'\n",
    "experiment_instance.calc_reward(2)\n",
    "\n",
    "print 'Deck 3:'\n",
    "experiment_instance.calc_reward(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from code import __init__\n",
    "\n",
    "We've added this to our code. We have a seperate file, 'code.py', that contains all the functions we will write. We wrote '\\__init\\__' in the previous section. To save space, we can replace the entire '\\__init\\__' with 'from code import \\__init\\__', which takes '\\__init\\__' from code.py and puts it where the import line is.\n",
    "\n",
    "We will be adding more functions to this line throughout the tutorial. Functions can be added by separating them with a comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, we've now added the 'calc_reward' function to our code. Let's walk through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def calc_reward(self, deck_to_draw):\n",
    "\n",
    "This function has two parameters (something you must give the function for it to run).\n",
    "\n",
    "The first parameter is 'self'. This is so the functions knows what instance to compute things on. All our functions in this class will need 'self'.\n",
    "\n",
    "The other parameter is the deck number from which to draw a card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    deck_number = self.deck_values[deck_to_draw]\n",
    "    \n",
    "This line of code gets the base deck number from the deck_to_draw input parameter, and calls it 'deck_number'. We use the deck_number to 'access' the 'base deck'.\n",
    "\n",
    "For example, if the base decks are randomised in the order [2, 0, 3, 1]:\n",
    "\n",
    "    self.deck_values[0]\n",
    "    \n",
    "would return 2, as 2 is in position 0 in the list. 0 will be the number used to access the base deck 2.\n",
    "\n",
    "    self.deck_values[3]\n",
    "    \n",
    "would return 1, as 1 is in position 3 in the list.\n",
    "\n",
    "As specified above, we will call the randomised deck numbers the 'base decks'. We will call the numbers used to access them as decks zero through three. With the randomised order above, deck 1 will refer to base deck 2. Deck 1 will refer to base deck 0. Deck 2 will refer to base deck 3. Deck 3 will refer to base deck 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    good = 0\n",
    "\n",
    "This is an integer that we will use to return if the deck chosen is a 'good' deck, that will win money over time, or a 'bad' deck, that will lose money over time. We will return +1 for 'good' decks, and -1 for 'bad' decks.\n",
    "\n",
    "    rand_num = random.random()\n",
    "    \n",
    "This calls the 'random' function from the 'random' library. This generates a random float (a decimal number, non integer) between 0.0 and 1.0 which we will use to decide rewards later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    if deck_number == 0:\n",
    "\n",
    "        good = -1 \n",
    "        \n",
    "        if rand_num < 0.5:\n",
    "            reward = -150\n",
    "        else:\n",
    "            reward = 100\n",
    "            \n",
    "Let's walk through this line by line.\n",
    "\n",
    "An 'if' statement means that if the given condition is met, we run the following code that is indented.\n",
    "With our code, if the deck we have picked has a deck number of 0 (== means 'is equivalent to'), we enter this 'if' statement. \n",
    "\n",
    "We set good to -1 as we will make it a 'bad' deck, with a net loss over time.\n",
    "\n",
    "We then use our random number. If the random number is less than 0.5, we set reward to -150. Otherwise, we set reward to +100. This gives a 50% chance of losing 150, and a 50% chance of winning 100. On average, over 10 draws, this deck will lose 250.\n",
    "\n",
    "We use similar statements for the other three decks. 'elif' stands for 'else if'. If the previous 'if' statement was not satisfied, the condition is checked against this statement. If it satisfies it, we enter this statement.\n",
    "\n",
    "The decks correspond to the decks used by Bull et al., 2015, copying Bechara et al., 1994:\n",
    "\n",
    "Decks 0 and 1 have a net loss of \\$250 over 10 plays. Deck 2 and 3 have a net gain of \\$200 and \\$250 respectively over 10 plays. Decks 0 and 2 have frequent, but small loses. Decks 1 and 3 have infrequent but large losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    return reward, good\n",
    "\n",
    "We return two things: the reward, and if the deck is 'good' or 'bad' to wherever called this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code will generate the decks, shuffle them, and draw a card from each deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Which Deck to Draw From"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting this section of code, we need to understand the algorithm used to choose the deck to draw from each time, called 'epsilon-greedy'.\n",
    "\n",
    "\n",
    "Each time the algorithm faces a choice of which deck to choose, it must decide whether to draw from the one it currently thinks is best, called exploitation, or draw from a different one in order to test if it might be better, called exploration.\n",
    "\n",
    "\n",
    "The epsilon value is the parameter we change. This changes that the amount the algorithm chooses to exploit vs explore. The epsilon value is the chance the algorithm chooses to explore rather than exploit, i.e. the chance it chooses another random cdeck.\n",
    "\n",
    "\n",
    "For example, choosing an epsilon value of 0.0 means that it never chooses to explore, it will always just choose the deck that it thinks is best, while an epsilon value of 1.0 means it will always randomly choose a deck that is not the best.\n",
    "\n",
    "Let's take a look at the code, and see how it works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Iowa:\n",
    "\n",
    "    from code import __init__, calc_reward\n",
    "\n",
    "    # choose which deck to draw from\n",
    "    def choose_eps_greedy(self, epsilon):  \n",
    "\n",
    "        # generate a random float between 0.0 and 1.0\n",
    "        rand_num = random.random()  \n",
    "\n",
    "        # get best deck\n",
    "        best_deck = np.argmax(self.est_values)\n",
    "\n",
    "        # if the random number is higher than epsilon, exploit\n",
    "        if rand_num > epsilon:  \n",
    "            print \"Exploit: \", best_deck\n",
    "\n",
    "            return best_deck  \n",
    "\n",
    "        # if random number is lower than epsilon, explore\n",
    "        else:\n",
    "            random_deck = random.choice(self.deck_values)\n",
    "\n",
    "            # make sure random deck chosen is not best deck\n",
    "            while random_deck == best_deck:\n",
    "                random_deck = random.choice(self.deck_values)\n",
    "\n",
    "            print \"Explore: \", random_deck\n",
    "            return random_deck \n",
    "        \n",
    "experiment_instance = Iowa()  \n",
    "experiment_instance.choose_eps_greedy(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def choose_eps_greedy(self, epsilon)\n",
    "\n",
    "The function takes in epsilon as a parameter, so it can choose the amount it exploits vs explores. Epsilon is a float between 0.0 and 1.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    rand_num = random.random()\n",
    "\n",
    "This generates a random float between 0.0 and 1.0, like we used in the previous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    best_deck = np.argmax(self.est_values)\n",
    "    \n",
    "The argmax function is called from the numpy library. It returns the index of the highest value in the list of est_values.\n",
    "\n",
    "est_values contains the estimated values of each of the decks (as discussed above).\n",
    "\n",
    "Therefore, the current best deck is returned.\n",
    "\n",
    "For example, if the est_values list is [0, -100, 50, 100], 3 will be returned. 100 is the highest value, and 3 is the index at which it is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if rand_num > epsilon:\n",
    "        return best_deck\n",
    "        \n",
    "If the random number is higher than the random number generated, exploit by choosing the deck that is currently thought to be best.\n",
    "\n",
    "    else:\n",
    "        random_deck = random.choice(self.deck_values)\n",
    "\n",
    "        while random_deck == best_deck:\n",
    "            random_deck = random.choice(self.deck_values)\n",
    "\n",
    "        return random_deck\n",
    "\n",
    "If the random number is not higher than epsilon, explore by randomly choosing a deck to draw from. For example, if epsilon is 0.2, then there will be a 20% chance of exploring.\n",
    "\n",
    "    random_deck = random.choice(self.deck_values)\n",
    "    \n",
    "This randomly chooses a deck from the possible decks.\n",
    "\n",
    "    while random_deck == best_deck:\n",
    "        random_deck = random.choice(self.deck_values)\n",
    "            \n",
    "If the random deck is the same as the best deck, we enter this while loop. We keep choosing a new random deck until it is not the same as the best deck. This prevents the best deck being chosen when we are exploring.\n",
    "\n",
    "    return random_deck\n",
    "    \n",
    "We return this random deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each time we draw a card, we need to update the estimates for what we think the deck value (expected return) is. We do this by calculating an average of every card drawn from that deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Iowa:\n",
    "    \n",
    "    from code import __init__, calc_reward, choose_eps_greedy\n",
    "\n",
    "    def update_est(self, deck_to_draw, reward):\n",
    "        \n",
    "        # increase number of deck draws by one\n",
    "        self.num_draws[deck_to_draw] += 1  \n",
    "        \n",
    "        # calculate the step-size\n",
    "        alpha = 1./self.num_draws[deck_to_draw] \n",
    "        \n",
    "        # running average of rewards\n",
    "        self.est_values[deck_to_draw] += alpha * (reward - self.est_values[deck_to_draw])  \n",
    "        print \"Updated average: \", self.est_values[deck_to_draw]\n",
    "        \n",
    "experiment_instance = Iowa()  \n",
    "experiment_instance.update_est(0, 100)\n",
    "experiment_instance.update_est(0, -150)\n",
    "experiment_instance.update_est(0, -150)\n",
    "experiment_instance.update_est(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def update_est(self, deck_to_draw, reward)\n",
    "    \n",
    "The function takes in the deck number we drew from, and the reward received as parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.num_draws[deck_to_draw] += 1\n",
    "\n",
    "num_draws is the list storing the number of times each deck has been drawn from. We increment the value stored at the index corresponding to the deck that was drawn from after each draw. The above line is the same as:\n",
    "    \n",
    "    self.num_draws[deck_to_draw] = self.num_draws[deck_to_draw] + 1\n",
    "\n",
    "For example, if a card is drawn from deck 0, self.num_draws[0] will be incremented by one, i.e. if the num_draws list was [12, 32, 25, 29], the list would become [13, 32, 25, 29]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    alpha = 1./self.num_draws[deck_to_draw]\n",
    "\n",
    "This calculates the step-size, something we use in the next step. It is calculated using: 1/(number of draws from that deck)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.est_values[deck_to_draw] += alpha * (reward - self.est_values[deck_to_draw]) \n",
    "\n",
    "This updates the average reward from the deck that was drawn from. It is an incremental way of calculating (total reward from deck)/(number of draws from deck), which is the average. The actual algorithm itself is not important, and there are lots of algorithms that do the same thing, but make sure to understand what it calculates.\n",
    "\n",
    "As an exercise, you could try implementing a different algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code will calculate the estimates if the first four rewards drawn from deck 0 were: 100, -150, -150, 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the code we need to run experiments, and we can finish off the class. This section of code is not runnable, we'll go through how to run it in the following section.\n",
    "\n",
    "Like the experiment conducted by Bull et al., 2015, we will be displaying our results in 20 draw blocks. In summary, we will count the number of 'good' draws minus 'bad' draws for the first 20 draws, and call that block one. We will then count the number of 'good' draws minus 'bad' draws for the next twenty blocks, and call that block two, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Iowa:\n",
    "    \n",
    "    from code import __init__, calc_reward, choose_eps_greedy, update_est\n",
    "    \n",
    "    def experiment(self, num_draws_wanted, epsilon, block_size):\n",
    "        \n",
    "        history = []\n",
    "        results = []\n",
    "        running_total = 0\n",
    "\n",
    "        for n in range(num_draws_wanted):\n",
    "            \n",
    "            deck_to_draw = self.choose_eps_greedy(epsilon)  # choose which deck to draw from\n",
    "            (reward, good_draw) = self.calc_reward(deck_to_draw)  # calculate the reward from drawing\n",
    "            self.update_est(deck_to_draw, reward)  # update deck value estimates\n",
    "            history.append(good_draw)  # store reward value\n",
    "\n",
    "        for n in range(len(history)):\n",
    "\n",
    "            if n%block_size == 0:  # if at the start of a new block\n",
    "                results.append(running_total)  # append the number of net good draws to results\n",
    "                running_total = 0  # reset running total to zero\n",
    "\n",
    "            running_total += history[n]\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the final part of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def experiment(self, num_draws_wanted, epsilon, block_size):\n",
    "    \n",
    "The parameters are num_draws_wanted (the total number of draws wanted), epsilon (the epsilon value wanted between 0.0 and 1.0), and block_size.\n",
    "\n",
    "Bull et al., 2015 used 200 draws per participant, split into 10 blocks of 20 draws, and calculated the net 'good' draws for each block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    history = []\n",
    "\n",
    "This creates an empty list that we will use to store the history of if the draw was from a 'good' or 'bad' deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    results = []\n",
    "\n",
    "This creates an empty list that we will use to store the net number of good draws per block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    running_total = 0\n",
    "\n",
    "This is an integer we will use to keep a running total of good draws per block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    for n in range(num_draws_wanted):\n",
    "    \n",
    "        deck_to_draw = experiment_instance.choose_eps_greedy(epsilon)\n",
    "        (reward, good_draw) = experiment_instance.calc_reward(deck_to_draw)\n",
    "        experiment_instance.update_est(deck_to_draw, reward)\n",
    "        history.append(good_draw)  \n",
    "\n",
    "We input the number of draws we want as a parameter, and we then use that in a 'for' loop. A 'for' loop will run the following code the inputed number of times, in this case, 'num_draws_wanted'.\n",
    "\n",
    "Each loop does this:\n",
    "\n",
    "First calculates the deck to draw from using our choose_eps_greedy function.\n",
    "\n",
    "Next, we calculate the reward, and if the deck is 'good' or 'bad' using our calc_reward function. Remember that the function has two returns.\n",
    "\n",
    "We then update the estimate for that deck using our update_est function.\n",
    "\n",
    "    list0.append(list1)\n",
    "    \n",
    "This adds list1 to the end of list0.\n",
    "\n",
    "We then append whether it was a 'good' or 'bad draw (+1 or -1) to the end of our history list. The history list will be a list of '-1's and '1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    for n in range(len(history)):\n",
    "\n",
    "            if n%block_size == 0:\n",
    "                results.append(running_total)\n",
    "                running_total = 0\n",
    "\n",
    "            running_total += history[n]\n",
    "\n",
    "        return results\n",
    "        \n",
    "This code now splits our history list into blocks.\n",
    "\n",
    "It loops through the history list.\n",
    "    \n",
    "    running_total += history[n]\n",
    "\n",
    "counts the number of 'good' draws - 'bad' draws, the net 'good' draws.\n",
    "    \n",
    "For example, if the first five elements of the history list are [1, -1, -1, 1, -1], the running total will become (in order): 1, 0, -1, 0, -1.\n",
    "\n",
    "If it reaches the start of a new block, it will append this total to the end of our results list, and reset that total to zero ready for the next block. It calculates if it has reached a new block using the modulo function, %. The modulo function returns the remainder when one number is divided by another. \n",
    "\n",
    "Using a block size of 20, as in the experiment, when it reaches the 20th element of the history list, modulo will be 0, as 20/20 is 1, remainder 0, and therefore it knows it is at the start of a new block.\n",
    "\n",
    "If it is at the 55th element of the history list, modulo will be 15, as 55/20 is 2, remainder 15, so it knows it is not at the start of a new block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We've finished the class. Let's talk through how to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiment function to get some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Iowa:\n",
    "    \n",
    "    from code import __init__, calc_reward, choose_eps_greedy, update_est, experiment\n",
    "    \n",
    "experiment_instance = Iowa()\n",
    "outcome = experiment_instance.experiment(20, 0.1, 4)  # run the experiment\n",
    "print outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    experiment_instance = Iowa()\n",
    "    outcome = experiment_instance.experiment(20, 0.1, 4) \n",
    "\n",
    "Running the code above draws a card 20 times, with an epsilon of 0.1, and a block size of 4. The net 'good' draws for each block are listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Running the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code we will use to run the experiment. This section is not runnable as we need to add it to our main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block_size = 20\n",
    "num_runs = 50\n",
    "num_draws = 200\n",
    "\n",
    "avg_outcome_eps0p0 = np.zeros(num_draws/block_size)  # store the results of each run\n",
    "avg_outcome_eps0p01 = np.zeros(num_draws/block_size)\n",
    "avg_outcome_eps0p1 = np.zeros(num_draws/block_size)\n",
    "avg_outcome_eps0p25 = np.zeros(num_draws/block_size)\n",
    "\n",
    "for i in range(num_runs):\n",
    "\n",
    "    experiment_instance = Iowa()\n",
    "    avg_outcome_eps0p0 += experiment_instance.experiment(num_draws, 0.0, block_size)\n",
    "\n",
    "    experiment_instance = Iowa()\n",
    "    avg_outcome_eps0p01 += experiment_instance.experiment(num_draws, 0.01, block_size)\n",
    "\n",
    "    experiment_instance = Iowa()\n",
    "    avg_outcome_eps0p1 += experiment_instance.experiment(num_draws, 0.1, block_size)\n",
    "\n",
    "    experiment_instance = Iowa()\n",
    "    avg_outcome_eps0p25 += experiment_instance.experiment(num_draws, 0.25, block_size)\n",
    "\n",
    "avg_outcome_eps0p0 /= np.float(num_runs)  # average the results\n",
    "avg_outcome_eps0p01 /= np.float(num_runs)\n",
    "avg_outcome_eps0p1 /= np.float(num_runs)\n",
    "avg_outcome_eps0p25 /= np.float(num_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    block_size\n",
    "    num_runs\n",
    "    num_draws\n",
    "These set up the block size wanted, the number of runs of the experiment wanted (i.e. the number of participants), and the number of deck draws for each run.\n",
    "\n",
    "We have set the block size to 20, the number of runs to 50, and the number of draws to 200, to mirror the Bull et al. experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    avg_outcome_eps0p0 = np.zeros(num_draws/block_size)\n",
    "    avg_outcome_eps0p01 = np.zeros(num_draws/block_size)\n",
    "    avg_outcome_eps0p1 = np.zeros(num_draws/block_size)\n",
    "    avg_outcome_eps0p25 = np.zeros(num_draws/block_size)\n",
    "\n",
    "These are arrays (similar to lists) to store the results of the net 'good' draws per block. The number of blocks needed is num_draws/block_size. np.zeros creates arrays of the given size and makes all the elements zero.\n",
    "    \n",
    "    eps0p0\n",
    "\n",
    "reads epsilon zero point zero (0.0)\n",
    "\n",
    "    eps0p25\n",
    "    \n",
    "reads epsilon zero point two five (0.25)\n",
    "\n",
    "The epsilon values we use are 0.0, 0.01, 0.1, and 0.25. 0.0 is picked so we can see the results purely from picking what we think is the best value. The others are there to provide a wide variety of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    for i in range(num_runs):\n",
    "\n",
    "        experiment_instance = Iowa()\n",
    "        avg_outcome_eps0p0 += experiment_instance.experiment(num_draws, 0.0, block_size)\n",
    "\n",
    "        experiment_instance = Iowa()\n",
    "        avg_outcome_eps0p01 += experiment_instance.experiment(num_draws, 0.01, block_size)\n",
    "\n",
    "        experiment_instance = Iowa()\n",
    "        avg_outcome_eps0p1 += experiment_instance.experiment(num_draws, 0.1, block_size)\n",
    "\n",
    "        experiment_instance = Iowa()\n",
    "        avg_outcome_eps0p25 += experiment_instance.experiment(num_draws, 0.25, block_size)\n",
    "\n",
    "This runs the experiment, for the given number of runs.\n",
    "\n",
    "The parameters for each experiment instance are: the given number of draws, the given epsilon value, and given block size. It adds the results after each run to the end of the current avg_outcome for that experiment instance.\n",
    "\n",
    "For example, if the current avg_outcome for a given epsilon value is [-2, 0, 2...] and the next results array for the same epsilon value is [0, 3, 5...], adding them together would result in [-2, 3, 7...]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    avg_outcome_eps0p0 /= np.float(num_runs) \n",
    "    avg_outcome_eps0p01 /= np.float(num_runs)\n",
    "    avg_outcome_eps0p1 /= np.float(num_runs)\n",
    "    avg_outcome_eps0p25 /= np.float(num_runs)\n",
    "\n",
    "This averages the results. The net 'good' draws in the first block, among all the participants is averaged, the net 'good' draws in the second block, among all the participants is averaged, and so on.\n",
    "\n",
    "So if the results array is currently [-2, 3, 7...] and the number of runs is 2, the end result will be [-1, 1.5, 3.5...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section can be a bit confusing so a worded explanation might help:\n",
    "\n",
    "There are four separate experiments. One with an epsilon value of 0.0, one with an epsilon value of 0.01, one with an epsilon value of 0.1, and one with an epsilon value of 0.25.\n",
    "\n",
    "Each experiment is where 200 cards are drawn. Each experiment is run 50 times.\n",
    "\n",
    "The 200 card draws are split into 'blocks' of 20 draws, and the number of 'good' - 'bad' draws within each block is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Result Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code we will use to plot graphs of our results. This section of code is not runnable, as we need to add the rest of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(avg_outcome_eps0p0, label=\"eps = 0.0\")  # plot the results, and give a label\n",
    "plt.plot(avg_outcome_eps0p01, label=\"eps = 0.01\")\n",
    "plt.plot(avg_outcome_eps0p1, label=\"eps = 0.1\")\n",
    "plt.plot(avg_outcome_eps0p25, label=\"eps = 0.25\")\n",
    "plt.ylim(-10, 20)  # limits for the y-axis\n",
    "plt.legend(loc='best')  # display the graph key\n",
    "plt.savefig('graph.png')  # save graph to folder\n",
    "plt.show()  # display the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly add\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "to the top of our code as we will be using this library to plot result graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     plt.plot(results_to_be_plotted, label=\"label wanted\")\n",
    "\n",
    "This plots the results calculated above, and labels these results. We plot a graph for each epsilon value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    plt.ylim(-10, 25)\n",
    "\n",
    "This restricts the y-axis from -10 to 25. The maximum number of 'good' draws is 20, the block size. We use 25 to give some space at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    plt.savefig('graph.png')\n",
    "\n",
    "This saves a copy of the graph to the same folder that this notebook is stored in, as 'graph.png'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    plt.legend(loc = 'best') \n",
    "\n",
    "This provides the graph with a key to associate each coloured line with the correct label, and places it where the function determines to be best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    plt.show()\n",
    "\n",
    "This displays the graph on screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvhBCeAkJfCGE8BIS+EII4SX+H6sXZ7mUqfZqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4a2ec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Iowa:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.deck_values = [0, 1, 2, 3]\n",
    "\n",
    "        random.shuffle(self.deck_values)  # randomise deck order\n",
    "\n",
    "        self.num_draws = [0, 0, 0, 0]  # set up list for tracking number of draws\n",
    "        self.est_values = [0, 0, 0, 0]  # set up list for tracking estimated values\n",
    "\n",
    "    def calc_reward(self, deck_to_draw):\n",
    "\n",
    "        deck_number = self.deck_values[deck_to_draw]  # the deck number to draw from\n",
    "\n",
    "        good = 0  # if the deck is good or bad\n",
    "\n",
    "        rand_num = random.random()  # random float between 0.0 and 1.0\n",
    "\n",
    "        if deck_number == 0:  # average over 10 draws, -250\n",
    "\n",
    "            good = -1  # bad deck\n",
    "\n",
    "            if rand_num < 0.5:\n",
    "                reward = -150\n",
    "            else:\n",
    "                reward = 100\n",
    "\n",
    "        elif deck_number == 1:  # average over 10 draws, -250\n",
    "\n",
    "            good = -1  # bad deck\n",
    "\n",
    "            if rand_num < 0.1:\n",
    "                reward = -700\n",
    "            else:\n",
    "                reward = 50\n",
    "\n",
    "        elif deck_number == 2:  # average over 10 draws, +200\n",
    "\n",
    "            good = 1  # good deck\n",
    "\n",
    "            if rand_num < 0.5:\n",
    "                reward = -35\n",
    "            else:\n",
    "                reward = 75\n",
    "\n",
    "        elif deck_number == 3:  # average over 10 draws, +250\n",
    "\n",
    "            good = 1  # good deck\n",
    "\n",
    "            if rand_num < 0.1:\n",
    "                reward = -200\n",
    "            else:\n",
    "                reward = 50\n",
    "\n",
    "        return reward, good\n",
    "\n",
    "    # choose which deck to draw from\n",
    "    def choose_eps_greedy(self, epsilon):  \n",
    "\n",
    "        # generate a random float between 0.0 and 1.0\n",
    "        rand_num = random.random()  \n",
    "\n",
    "        # get best deck\n",
    "        best_deck = np.argmax(self.est_values)\n",
    "\n",
    "        # if the random number is higher than epsilon, exploit\n",
    "        if rand_num > epsilon:  \n",
    "            \n",
    "            return best_deck  \n",
    "\n",
    "        # if random number is lower than epsilon, explore\n",
    "        else:\n",
    "            random_deck = random.choice(self.deck_values)\n",
    "\n",
    "            # make sure random deck chosen is not best deck\n",
    "            while random_deck == best_deck:\n",
    "                random_deck = random.choice(self.deck_values)\n",
    "\n",
    "            return random_deck \n",
    "\n",
    "    def update_est(self, deck_to_draw, reward):\n",
    "\n",
    "        # increase number of deck draws by one\n",
    "        self.num_draws[deck_to_draw] += 1  \n",
    "\n",
    "        # calculate the step-size\n",
    "        alpha = 1./self.num_draws[deck_to_draw] \n",
    "\n",
    "        # running average of rewards\n",
    "        self.est_values[deck_to_draw] += alpha * (reward - self.est_values[deck_to_draw])  \n",
    "\n",
    "    def experiment(self, num_draws_wanted, epsilon, block_size):\n",
    "\n",
    "        history = []\n",
    "        results = []\n",
    "        running_total = 0\n",
    "\n",
    "        for n in range(num_draws_wanted):\n",
    "\n",
    "            deck_to_draw = self.choose_eps_greedy(epsilon)  # choose which deck to draw from\n",
    "            (reward, good_draw) = self.calc_reward(deck_to_draw)  # calculate the reward from drawing\n",
    "            self.update_est(deck_to_draw, reward)  # update deck value estimates\n",
    "            history.append(good_draw)  # store reward value\n",
    "\n",
    "        for n in range(len(history)):\n",
    "\n",
    "            if n%block_size == 0:  # if at the start of a new block\n",
    "                results.append(running_total)  # append the number of net good draws to results\n",
    "                running_total = 0  # reset running total to zero\n",
    "\n",
    "            running_total += history[n]\n",
    "\n",
    "        return results\n",
    "\n",
    "block_size = 20\n",
    "num_runs = 50\n",
    "num_draws = 200\n",
    "\n",
    "avg_outcome_eps0p0 = np.zeros(num_draws/block_size)  # store the results of each run\n",
    "avg_outcome_eps0p01 = np.zeros(num_draws/block_size)\n",
    "avg_outcome_eps0p1 = np.zeros(num_draws/block_size)\n",
    "avg_outcome_eps0p25 = np.zeros(num_draws/block_size)\n",
    "\n",
    "for i in range(num_runs):\n",
    "\n",
    "    experiment_instance = Iowa()\n",
    "    avg_outcome_eps0p0 += experiment_instance.experiment(num_draws, 0.0, block_size)\n",
    "\n",
    "    experiment_instance = Iowa()\n",
    "    avg_outcome_eps0p01 += experiment_instance.experiment(num_draws, 0.01, block_size)\n",
    "\n",
    "    experiment_instance = Iowa()\n",
    "    avg_outcome_eps0p1 += experiment_instance.experiment(num_draws, 0.1, block_size)\n",
    "\n",
    "    experiment_instance = Iowa()\n",
    "    avg_outcome_eps0p25 += experiment_instance.experiment(num_draws, 0.25, block_size)\n",
    "\n",
    "avg_outcome_eps0p0 /= np.float(num_runs)  # average the results\n",
    "avg_outcome_eps0p01 /= np.float(num_runs)\n",
    "avg_outcome_eps0p1 /= np.float(num_runs)\n",
    "avg_outcome_eps0p25 /= np.float(num_runs)\n",
    "\n",
    "# plot results\n",
    "plt.plot(avg_outcome_eps0p0, label=\"eps = 0.0\")  # plot the results, and give a label\n",
    "plt.plot(avg_outcome_eps0p01, label=\"eps = 0.01\")\n",
    "plt.plot(avg_outcome_eps0p1, label=\"eps = 0.1\")\n",
    "plt.plot(avg_outcome_eps0p25, label=\"eps = 0.25\")\n",
    "plt.ylim(-10, 25)  # limits for the y-axis\n",
    "plt.legend(loc='best')  # display the graph key\n",
    "plt.savefig('graph.png')  # save graph to folder\n",
    "plt.show()  # display the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the completed code to see the results.\n",
    "\n",
    "You can also change the epsilon values, and see how that changes results.\n",
    "\n",
    "Congratulations! You've successfully build a model of the Iowa Gambling Task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we've got the results, let's see what they mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here's a graph of results for block size 20, with 50 participants, and 200 draws, as in the Bull et al. experiment. You should see results similar to this.\n",
    "\n",
    "\n",
    "<img src=\"graph.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The graph shows the number of net 'good' draws in each block for each epsilon value.\n",
    "\n",
    "As we can see, all algorithms get better over time, showing our epsilon-greedy algorithm works, but the rate at which it improves, and the value at which it plateaus changes depending on our choice of epsilon value.\n",
    "\n",
    "Let's compare it to the results of the paper.\n",
    "\n",
    "<img src=\"PaperGraph.png\">\n",
    "\n",
    "Our y-axis matches the right y-axis in the graph from the paper (the net positive draws).\n",
    "\n",
    "The line plotted for the graph from the paper matches an epsilon value of 0.25 well. This suggests that humans make choices using a similar method to an epsilon-greedy algorithm with an epsilon value of 0.25.\n",
    "\n",
    "While an epsilon value matches humans, it is not the best result; all the other values give faster learning rates, and a higher plateau. This becomes especially important in similar problems with more choices (more decks of cards).\n",
    "\n",
    "Setting the epsilon value to 0.1 means that the algorithm will choose to explore 10% of the time. This allows it to explore quite quickly, while exploiting this exploration the vast majority of time. This means this algorithm improves quickly.\n",
    "\n",
    "Setting the epsilon value to 0.01 means the algorithm will only choose to explore 1% of the time. This means it improves quite slowly at the beginning, as the algorithm needs to run quite a few times before it obtains good exploration results. However, this value beats 0.1 in the long term, as once the algorithm has explored enough and has enough information about each of the decks, it will only draw from a deck that isn't the 'best' 1% of the time, while with an epsilon value of 0.1, a deck that isn't the 'best' will be drawn from 10% of the time.\n",
    "\n",
    "With this particular experiment, an epsilon value of 0.0, known as a 'greedy' algorithm actually performs best, as once it learns the 'best' deck, it will always pick it, and no choose any other deck. However, this would not be suitable for problems with decks that vary over time.\n",
    "\n",
    "This all means that in larger problems, a higher epsilon value at the beginning is the best, as it allows quick exploration, while a lower epsilon value the more the algorithm runs is the best, as this allows it to exploit more often once it has established the deck values. Some algorithms implement an epsilon value that starts higher, and slowly lowers over time, taking advantage of this fact.\n",
    "\n",
    "Try changing the epsilon values to see varying results, and as an exercise, try implementing an epsilon value that slowly decreases over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "Bechara, A., Damasio, A. R., Damasio, H. and Anderson, S. W. (1994), Insensitivity to future consequences following damage to human prefrontal cortex, Cognition 50(1), 715.\n",
    "\n",
    "Bull, P. N., Tippett, L. J. and Addis, D. R. (2015), Decision making in healthy participants on the iowa gambling task: new insights from an operant approach, Frontiers in psychology 6, 391.\n",
    "\n",
    "Farris, B. (2016), Multi-armed bandits, http://blog.thedataincubator.com/2016/07/multi-armed-bandits-2/. [Online; accessed 16 January 2017].\n",
    "\n",
    "Hamrick, J. (2011), An Introduction to Classes and Inheritance (in Python), http://www.jesshamrick.com/2011/05/18/an-introduction-to-classes-and-inheritance-in-python/. [Online; accessed 09 March 2017].\n",
    "\n",
    "TutorialPoint. (n.d.), 'Python Lists', https://www.tutorialspoint.com/python/python_lists.htm. [Online; accessed 09 March 2017]."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
