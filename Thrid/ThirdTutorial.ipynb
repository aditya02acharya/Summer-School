{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Decision Making in Rats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third tutorial will be based on the same problem as the second tutorial.\n",
    "\n",
    "Previously, the rat knew what state it was in. Imagine now this information is not available to the rat. It is able to make observations of its surroundings so it can estimate likelihoods of being in each state, but does not know for sure.\n",
    "\n",
    "This is the basis of the next tutorial.\n",
    "\n",
    "The rat holds what is known as a 'belief state', a probability distribution. This is the probability the rat thinks it is in each state. Since this is a probability distribution, the sum of all the probabilities must be 1, though there are exceptions that we will talk about later. Brown University, 1999, provides some good information on belief states.\n",
    "\n",
    "Here is a reminder of the state transitions:\n",
    "\n",
    "<img src=\"states.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial has been produced using Jupyter Notebook. You will sections of text, accompanied by sections of code. To run the code, click on it, hold shift, and press enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's begin writing the code. \n",
    "\n",
    "Each section in the tutorial will begin with a block of code, followed by a line by line explanation of the code. The code most likely won't make much sense before reading the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "class Maze:\n",
    "    def __init__(self):\n",
    "\n",
    "        # array for storing reward values\n",
    "        self.r = np.array([[-1, -10, -30,  -1],   # State S0\n",
    "                           [-1,  -1, -30,  90],   # State S1\n",
    "                           [-1,  -1,  -1,  -1],   # State S2\n",
    "                           [-1,  -1,  -1,  -1]])  # State S3\n",
    "\n",
    "        # current reward\n",
    "        self.reward = -1\n",
    "        \n",
    "        # keep track of total reward \n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        # list of states\n",
    "        self.state_list = [0, 1, 2, 3]\n",
    "\n",
    "        # belief state, all same to start\n",
    "        self.belief_state = [0.3, 0.3, 0.3, 0.3]  # Choice to be made in s0 and s1\n",
    "\n",
    "        # set up belief table\n",
    "        self.belief_table = []\n",
    "\n",
    "        for s0 in range(0, 11):\n",
    "            for s1 in range (0, 11):\n",
    "                for s2 in range (0, 11):\n",
    "                    for s3 in range (0, 11):\n",
    "                        self.belief_table.append([s0, s1, s2, s3, 0, 0])\n",
    "\n",
    "        # convert to array\n",
    "        self.belief_table = np.vstack(self.belief_table)\n",
    "        \n",
    "        # divide all by 10\n",
    "        self.belief_table = self.belief_table/10.0\n",
    "\n",
    "        # index of belief table corresponding to belief state\n",
    "        self.belief_table_index = -1\n",
    "\n",
    "        # index of belief table corresponding to previous belief state\n",
    "        self.previous_belief_table_index = -1\n",
    "\n",
    "        # keep track of real state\n",
    "        self.real_state = -1\n",
    "\n",
    "        # action number: 0 = Press lever, 1 = Enter magazine\n",
    "        self.action_number = -1\n",
    "\n",
    "        # state to move to next\n",
    "        self.next_state = -1\n",
    "        \n",
    "        # keep track of expected value\n",
    "        self.expected_value_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three lines are the same imports as the first two tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "This suppresses scientific notation when printing, for example, instead of printing 1.62300000e+03, 1623 will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, a class is created next. Classes hold functions that we will create. We have named our class 'Maze'.\n",
    "\n",
    "An initial '\\__init\\__' function has been created. This is used to set up the experiment initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.r = np.array([[-1,  -10,  -30,   -1],   # State S0\n",
    "                       [-1,   -1,  -30,   90],   # State S1\n",
    "                       [-1,   -1,   -1,   -1],   # State S2\n",
    "                       [-1,   -1,   -1,   -1]])  # State S3\n",
    "\n",
    "This sets up the reward array the same way as the second tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.reward = -1\n",
    "    \n",
    "This will hold a reward value that we get from the table when we move to a new state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.total_reward = 0.0\n",
    "        \n",
    "This will be used to keep track of total reward achieved throughout the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.state_list = [0, 1, 2, 3]\n",
    "    \n",
    "This is a list of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "    \n",
    "This is the belief state that we spoke about earlier. We start at having equal beliefs for each state, i.e. [0.25, 0.25, 0.25, 0.25], but we will be rounding everything to 1 decimal place, so these each get rounded up to 0.3. This is an example of an exception that we spoke about, where the sum is over 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.belief_table = []\n",
    "\n",
    "    for s0 in range(0, 11):\n",
    "        for s1 in range (0, 11):\n",
    "            for s2 in range (0, 11):\n",
    "                for s3 in range (0, 11):\n",
    "                    self.belief_table.append([s0, s1, s2, s3, 0, 0])\n",
    "                    \n",
    "    self.belief_table = np.vstack(self.belief_table)\n",
    "\n",
    "    self.belief_table = self.belief_table/10.0\n",
    "\n",
    "\n",
    "With each individual belief state, there are Q-values for each action that can be taken in that state. In our experiment, the two actions are 'Press lever' and 'Enter magazine', which we will refer to as A0 and A1 respectively. \n",
    "\n",
    "We will use a 'belief table' to store belief states along with these Q-values. This means that a single entry in this table will be [Probability in S0, Probability in S1, Probability in S2, Probability in S3, Q-value for A0, Q-value for A1].\n",
    "\n",
    "For example, entries could be: [0.4, 0.2, 0.3, 0.1, 20, 10] or [0.1, 0.5, 0.2, 0.2, 5, 30].\n",
    "\n",
    "We create an entry for every probability of 1 decimal place, ranging from 0.0 to 1.0 for each of the four states, and set the initial Q-values for A0 and A1 to 0. Here is a breakdown of the above code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.belief_table = []\n",
    "\n",
    "We create an empty list which we will populate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    for s0 in range(0, 11):\n",
    "        for s1 in range (0, 11):\n",
    "            for s2 in range (0, 11):\n",
    "                for s3 in range (0, 11):\n",
    "                    self.belief_table.append([s0, s1, s2, s3, 0, 0])\n",
    "                    \n",
    "\n",
    "This is an example of a nested 'for' loop. You should understand how basic 'for' loops work from the previous tutorials. Like normal 'for' loops, the inner code must be completed for the loop to increment by one. Therefore, for an outer 'for' loop to increment, all the inner code must be completed. Here's a more basic example to help:\n",
    "\n",
    "    example_list = []\n",
    "\n",
    "    for s0 in range (0, 3):\n",
    "        for s1 in range (0, 3):\n",
    "            self.example_list.append([s0, s1, 0])\n",
    "            \n",
    "This returns the result: [[0, 0, 0], [0, 1, 0], [0, 2, 0], [0, 3, 0], [1, 0, 0], [1, 1, 0], [1, 2, 0], [1, 3, 0], [2, 0, 0], [2, 1, 0], [2, 2, 0], [2, 3, 0], [3, 0, 0], [3, 1, 0], [3, 2, 0], [3, 3, 0]]\n",
    "\n",
    "This will append a list [s1, s2, 0] after each iteration of a loop.\n",
    "\n",
    "\n",
    "This creates all the entries in the belief table, matching all possible belief states, though using integers from 0 to 10 rather than 0.0 to 1.0. We originally use integers as we cannot use floats with 'for' loops.\n",
    "\n",
    "    self.belief_table = np.vstack(self.belief_table)\n",
    "\n",
    "This makes the changes the belief table from a list to an array, which will be easier for us later.\n",
    "\n",
    "    self.belief_table = self.belief_table/10.0\n",
    "    \n",
    "This divides all the entries by 10, reducing the range from 0 to 10, to 0.0 to 1.0, giving our final belief table.\n",
    "\n",
    "There are some entries that are not possible, such as [0.5, 0.5, 0.5, 0.5], as that entry sums to 2, but this is an easy way of creating the belief table, and we can just ignore and not use the entries that we don't need or are not possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.belief_table_index = -1\n",
    "    \n",
    "This keeps track of which entry in the belief table matches our belief state. \n",
    "\n",
    "For example, if the belief state is [0.3, 0.4, 0.2, 0.1], it will be used to store the index of the element in the belief table that contains the Q-values for A0 and A1 when this is the belief state, i.e. the index of the entry [0.3, 0.4, 0.2, 0.1, Q-value for A0, Q-value of A1].\n",
    "\n",
    "    self.previous_belief_table_index = -1\n",
    "\n",
    "This keeps track of the previous index. It will be used when we update the Q-values later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.real_state = -1\n",
    "    \n",
    "This is used to keep track of the real state that the rat is in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.action_number = -1 \n",
    "\n",
    "This is used to keep track of the action number chosen, A0 or A1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.next_state = -1\n",
    "\n",
    "The action number, together with the current state, will be used to calculate the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.expected_value_array = []\n",
    "    \n",
    "We will keep track of how the expected value changes over the course of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The reset function resets some values back to the original values. We will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "class Maze:\n",
    "    \n",
    "    from code import __init__\n",
    "\n",
    "    def reset(self, initial_state):\n",
    "\n",
    "        print \"Reset\"\n",
    "\n",
    "        self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "        self.real_state = initial_state\n",
    "        \n",
    "experiment_instance = Maze()\n",
    "experiment_instance.belief_state = [0.4, 0.2, 0.1, 0.3]\n",
    "experiment_instance.real_state = 2\n",
    "\n",
    "print \"Before reset\"\n",
    "print experiment_instance.belief_state\n",
    "print experiment_instance.real_state\n",
    "\n",
    "print \"\"\n",
    "experiment_instance.reset(0)\n",
    "\n",
    "print experiment_instance.belief_state\n",
    "print experiment_instance.real_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from code import __init__\n",
    "\n",
    "As in the first and second tutorials, we've added this to our code. We have a separate file, 'code.py', that contains all the functions we will write. We wrote '\\__init\\__' in the previous section. To save space, we can replace the entire '\\__init\\__' with 'from code import \\__init\\__', which takes '\\__init\\__' from code.py and puts it where the import line is.\n",
    "\n",
    "We will be adding more functions to this line throughout the tutorial. Functions can be added by separating them with a comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def reset(self, initial_state):\n",
    "\n",
    "We need the initial state as a parameter, so we know what to reset the state to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "    self.real_state = initial_state\n",
    "    \n",
    "We will use this function when the rat reaches a terminal state. These values are set back to what they were at the beginning, ready for a new run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to see how reset affects variables after it is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making an observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier, the rat can make an 'observation', which helps give an indication as to what state it is in. However, the rat cannot be sure its observation is correct. \n",
    "\n",
    "\n",
    "We model this by giving a chance of returning the real state that the rat is in when it performs an observation, otherwise an incorrect state will be returned. The chance it returns the correct state is known as the 'Observation Chance'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State observed:  0\n",
      "State observed:  0\n",
      "State observed:  0\n",
      "State observed:  0\n",
      "State observed:  0\n",
      "State observed:  0\n",
      "State observed:  1\n",
      "State observed:  0\n",
      "State observed:  0\n",
      "State observed:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "class Maze:\n",
    "    \n",
    "    from code import __init__, reset    \n",
    "    \n",
    "    def observe(self, observation_chance):\n",
    "\n",
    "        rand_num = random.random()\n",
    "        if rand_num < observation_chance:\n",
    "            return self.real_state\n",
    "        else:\n",
    "            random_state = random.choice(self.state_list)\n",
    "            \n",
    "            while random_state == self.real_state:\n",
    "                random_state = random.choice(self.state_list)\n",
    "                \n",
    "            return random_state\n",
    "        \n",
    "experiment_instance = Maze()\n",
    "experiment_instance.real_state = 0\n",
    "\n",
    "observation_chance = 0.65\n",
    "for n in range(0, 10):\n",
    "    observation = experiment_instance.observe(observation_chance)\n",
    "    print \"State observed: \", observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is quite simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def observe(self, observation_chance)\n",
    "\n",
    "We need the observation chance. This is the chance the observation returns the correct state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    rand_num = random.random()\n",
    "\n",
    "This generates a random float between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if rand_num < observation_chance:\n",
    "        return self.real_state\n",
    "\n",
    "If the random number is less than the observation chance, then the observation is 'correct' and returns the real state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    else:\n",
    "        random_state = random.choice(self.state_list)\n",
    "\n",
    "If the random number is not less than the observation chance, we select a random state from the state list.\n",
    "\n",
    "    while random_state == self.real_state:\n",
    "            random_state = random.choice(self.state_list)\n",
    "\n",
    "While this random_state is equal to the real state, we select a new random state. This stops the real state being returned, so if the observation 'fails', only an incorrect state can be returned.\n",
    "\n",
    "    return random_state\n",
    "\n",
    "We return this state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code will perform 10 observations while the rat is in S0. Run the code a few times to see the distribution of results. We've set the observation chance to 0.65 for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the belief state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the observation we need to update the belief state. We will do this using Bayes' rule:\n",
    "\n",
    "$$P(S_{j} | O_{k}) = \\dfrac {P(O_{k} | S_{j}) \\cdot P(S_{j-1})}{N}$$\n",
    "\n",
    "$P(S_{j} | O_{k})$ is the probability of the rat being in state j, given that the observation has returned k.\n",
    "\n",
    "In order to calculate this, we use Bayes' rule, which rewrites it in a way we can calculate.\n",
    "\n",
    "$P(O_{k} | S_{j})$ is the probability of observing k given that we are in state j. \n",
    "\n",
    "In our example, if given that the rat is in state 0, the chance of observing state 0 is our observation chance (0.65 in this example).\n",
    "\n",
    "If given that that the rat is in state 0, the chance of observing another state (1, 2 or 3) is 1 - observation chance.\n",
    "The chance of observing state 1 specifically is (1 - observation chance)/(total number of states - 1). The same applies to state 2 or 3.\n",
    "\n",
    "$P(S_{j-1})$ is the prior belief, the belief state before the  new observation.\n",
    "\n",
    "We divide by $N$, a normalising factor in order to make sure the probabilities sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "class Maze:\n",
    "    \n",
    "    from code import __init__, reset, observe  \n",
    "    \n",
    "    def update_belief_state(self, observation_state, observation_chance):\n",
    "\n",
    "        # Temporary list\n",
    "        temp_belief_state = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        # Use Bayes' Rule to update belief state\n",
    "        for n in range(len(temp_belief_state)):\n",
    "            if observation_state == n:\n",
    "                temp_belief_state[n] = observation_chance * self.belief_state[n]\n",
    "            else:\n",
    "                temp_belief_state[n] = ((1 - observation_chance)/(len(self.state_list) - 1)) * self.belief_state[n]\n",
    "                \n",
    "        print \"Before normalising \", temp_belief_state\n",
    "\n",
    "        # Normalise so the belief state sums to 1\n",
    "        temp_total = sum(temp_belief_state)\n",
    "        self.belief_state = list(n/temp_total for n in temp_belief_state)\n",
    "\n",
    "        # Round to 1 decimal place\n",
    "        self.belief_state = [round(n, 1) for n in self.belief_state]\n",
    "\n",
    "        print \"After update: \", self.belief_state\n",
    "        \n",
    "experiment_instance = Maze()\n",
    "experiment_instance.real_state = 0\n",
    "\n",
    "observation_chance = 0.65\n",
    "observation = experiment_instance.observe(observation_chance)\n",
    "print \"Observation: \", observation\n",
    "experiment_instance.update_belief_state(observation, observation_chance)\n",
    "\n",
    "observation = experiment_instance.observe(observation_chance)\n",
    "print \"\\nObservation: \", observation\n",
    "experiment_instance.update_belief_state(observation, observation_chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def update_belief_state(self, observation_state, observation_chance)\n",
    "    \n",
    "This function needs the observation and observation chance as parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    temp_belief_state = [0.0, 0.0, 0.0, 0.0]\n",
    "    \n",
    "We create a temporary belief state list that we will be changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    for n in range(len(temp_belief_state)):\n",
    "        if observation_state == n:\n",
    "            temp_belief_state[n] = observation_chance * self.belief_state[n]\n",
    "        else:\n",
    "            temp_belief_state[n] = ((1 - observation_chance)/(len(self.state_list) - 1)) * self.belief_state[n]\n",
    "    \n",
    "We loop through the temp_belief_state list, and update all states using the first part of Bayes' rule: $P(O_{k} | S_{j}) \\cdot P(S_{j-1})$\n",
    "\n",
    "    if observation_state == n:\n",
    "        temp_belief_state[n] = observation_chance * self.belief_state[n]\n",
    "\n",
    "If we are updating the belief the state returned by the observation.\n",
    "\n",
    "$P(O_{k} | S_{j})$ is the observation chance, as explained above.\n",
    "\n",
    "$P(S_{j-1})$ is self.belief_state[n], the prior belief. This gets the current belief that we are in that particular state (the belief before we've updated it). In this case, it will always be 0.3, as we've reset the belief state to [0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "    else:\n",
    "        temp_belief_state[n] = ((1 - observation_chance)/(len(self.state_list) - 1)) * self.belief_state[n]\n",
    "        \n",
    "If we are updating the belief of a state that is not the observed state:\n",
    "\n",
    "$P(O_{k} | S_{j})$ is (1 - the observation chance) divided by (len(self.state_list) - 1), as explained above.\n",
    "\n",
    "$P(S_{j-1})$ is self.belief_state[n], the same as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section calculates $N$ and normalises the belief state.\n",
    "    \n",
    "    temp_total = sum(temp_belief_state)\n",
    "    \n",
    "This is sum of all the elements in temp_belief_state, and is the normalising factor $N$.\n",
    "\n",
    "    self.belief_state = list(x/temp_total for x in temp_belief_state)\n",
    "    \n",
    "We divide every element in temp_belief_state by $N$, meaning temp_belief_state will sum to 1, and assign to belief_state.\n",
    "\n",
    "    self.belief_state = [round(x, 1) for x in self.belief_state]\n",
    "    \n",
    "We round all the elements in our belief state to 1 decimal place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code a few times to see the belief state update.\n",
    "\n",
    "The code will set the the real state to 0, perform an observation, update the belief state, perform another observation, and update the belief state again.\n",
    "\n",
    "If you change the way the belief state is reset, you may find that it is the case that the belief state does not sum to 1, as stated earlier. An example of this is [0.9, 0.0, 0.0, 0.0]. \n",
    "\n",
    "The real values may be [0.88, 0.04, 0.04, 0.04], which sums to 1, but after rounding to 1 decimal place, it becomes [0.9, 0.0, 0.0, 0.0]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the next action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code will choose the next action. We use the epsilon-greedy algorithm that we covered in first tutorial, and used again in the second tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.    0.    0.9   0.1  10.   20. ]\n",
      "Action chosen:  1\n",
      "Action chosen:  1\n",
      "Action chosen:  1\n",
      "Action chosen:  1\n",
      "Action chosen:  1\n",
      "Action chosen:  1\n",
      "Action chosen:  1\n",
      "Action chosen:  1\n",
      "Action chosen:  0\n",
      "Action chosen:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class Maze:\n",
    "\n",
    "    from code import __init__, reset, observe, update_belief_state\n",
    "\n",
    "    def choose_next_action(self, epsilon):\n",
    "\n",
    "        # get the q_values of the two actions\n",
    "        q_value_a0 = (self.belief_table[self.belief_table_index, 4])\n",
    "\n",
    "        q_value_a1 = (self.belief_table[self.belief_table_index, 5])\n",
    "\n",
    "        # choose an action using epsilon-greedy\n",
    "        rand_num = random.random()\n",
    "\n",
    "        if q_value_a0 > q_value_a1:\n",
    "\n",
    "            if rand_num > epsilon: \n",
    "                self.action_number = 0  # Exploit\n",
    "\n",
    "            else:  \n",
    "                self.action_number = 1  # Explore\n",
    "\n",
    "        elif q_value_a1 > q_value_a0:\n",
    "\n",
    "            if rand_num > epsilon:\n",
    "                self.action_number = 1  # Exploit\n",
    "\n",
    "            else:\n",
    "                self.action_number = 0  # Explore\n",
    "\n",
    "        else:\n",
    "            self.action_number = random.randint(0, 1)\n",
    "            \n",
    "        print \"Action chosen: \", self.action_number\n",
    "        \n",
    "experiment_instance = Maze()\n",
    "experiment_instance.belief_table_index = 100\n",
    "experiment_instance.belief_table[experiment_instance.belief_table_index, 4] = 10\n",
    "experiment_instance.belief_table[experiment_instance.belief_table_index, 5] = 20\n",
    "print experiment_instance.belief_table[experiment_instance.belief_table_index]\n",
    "\n",
    "epsilon = 0.1\n",
    "for n in range(10):\n",
    "    experiment_instance.choose_next_action(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def choose_next_action(self, epsilon):\n",
    "    \n",
    "We need the epsilon value as a parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    q_value_a0 = (self.belief_table[self.belief_table_index, 4])\n",
    "    \n",
    "Remember that each element in the belief table is laid out in the format:\n",
    "\n",
    "[Probability in S0, Probability in S1, Probability in S2, Probability in S3, Q-value for A0, Q-value for A1].\n",
    "\n",
    "The above line of code will get the value in the 4th position, which is the Q-value for A0 (Press lever).\n",
    "\n",
    "    q_value_a1 = (self.belief_table[self.belief_table_index, 5])\n",
    "    \n",
    "This similarly get the Q-value for A1 (Enter magazine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    rand_num = random.random()\n",
    "    \n",
    "Generate a random float between 0.0 and 1.0 to use in our epsilon-greedy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if q_value_a0 > q_value_a1:\n",
    "\n",
    "        if rand_num > epsilon:\n",
    "            self.action_number = 0  # Exploit\n",
    "\n",
    "        else:\n",
    "            self.action_number = 1  # Explore\n",
    "            \n",
    "If the Q-value for A0 is higher than the Q-value for A1:\n",
    "\n",
    "If the random number generated is higher than the epsilon value, we exploit, choosing A0, pressing the lever (the action we think is better).\n",
    "\n",
    "If it is lower, we explore, choosing A1, entering the magazine (the action we think it worse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    elif q_value_a1 > q_value_a0:\n",
    "\n",
    "        if rand_num > epsilon:\n",
    "            self.action_number = 1  # Exploit\n",
    "\n",
    "        else:\n",
    "            self.action_number = 0  # Explore\n",
    "            \n",
    "We do the same thing, but with the actions reversed if the Q-value for A1 is higher than the Q-value for A0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    else:\n",
    "        self.action_number = random.randint(0, 1)\n",
    "        \n",
    "Otherwise, the Q-values are equal, so we just randomly select an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code will set the Q-values of A0 and A1 for an arbitrary element (100) in belief_table to 10 and 20 respectively.\n",
    "\n",
    "The choose_next_action function will be run 10 times, with an epsilon value of 0.1, and an action will be picked. A1 should be picked significantly more than A0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to get the next state once we have the action number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  0\n",
      "Action number:  0\n",
      "Next state:  1\n",
      "\n",
      "Current state:  1\n",
      "Action number:  1\n",
      "Next state:  3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class Maze:\n",
    "\n",
    "    from code import __init__, reset, observe, update_belief_state, choose_next_action\n",
    "\n",
    "    # get corresponding next state from current state and action number\n",
    "    def get_next_state(self):\n",
    "\n",
    "        if self.real_state == 0:\n",
    "\n",
    "            if self.action_number == 0:\n",
    "\n",
    "                self.next_state = 1\n",
    "\n",
    "            elif self.action_number == 1:\n",
    "\n",
    "                self.next_state = 2\n",
    "\n",
    "        elif self.real_state == 1:\n",
    "\n",
    "            if self.action_number == 0:\n",
    "\n",
    "                self.next_state = 2\n",
    "\n",
    "            elif self.action_number == 1:\n",
    "\n",
    "                self.next_state = 3\n",
    "                \n",
    "        print \"Current state: \", self.real_state\n",
    "        \n",
    "        print \"Action number: \", self.action_number\n",
    "\n",
    "        print \"Next state: \", self.next_state\n",
    "\n",
    "\n",
    "experiment_instance = Maze()\n",
    "experiment_instance.real_state = 0\n",
    "experiment_instance.action_number = 0\n",
    "experiment_instance.get_next_state()\n",
    "\n",
    "print \"\"\n",
    "experiment_instance.real_state = 1\n",
    "experiment_instance.action_number = 1\n",
    "experiment_instance.get_next_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def get_next_state(self):\n",
    "    \n",
    "We don't need any additional parameters other than the experiment instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if self.real_state == 0:\n",
    "\n",
    "        if self.action_number == 0:\n",
    "\n",
    "            self.next_state = 1\n",
    "\n",
    "        elif self.action_number == 1:\n",
    "\n",
    "            self.next_state = 2\n",
    "\n",
    "Take a look at the image at the start of this tutorial to remind yourself of the state transitions.\n",
    "\n",
    "The code is pretty self-explanatory. If the current state is 0, and the rat has chosen A0 (pressing the lever), the rat moves into state 1. If the current state is 0, and the rat has chosen A1 (entering the magazine), the rat moves into state 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    elif self.real_state == 1:\n",
    "\n",
    "        if self.action_number == 0:\n",
    "\n",
    "            self.next_state = 2\n",
    "\n",
    "        elif self.action_number == 1:\n",
    "\n",
    "            self.next_state = 3\n",
    "            \n",
    "This adds the correct next_state calculations if the rat is currently in S1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code will get the next state if the current state is 0 and action 0 is chosen, and if the current state is 1 and action 1 is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the correct index of the belief table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gets the index in the belief table that corresponds to the belief state, i.e. if the belief state is [0.3, 0.3, 0.2, 0.2], this function will get the index of the belief table for the entry [0.3, 0.3, 0.2, 0.2, Q-value for A0, Q-value for A1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belief state:  [0.0, 0.0, 0.0, 0.0]\n",
      "Index:  [0]\n",
      "Belief table entry  [0] :  [[ 0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "Belief state:  [0.3, 0.4, 0.2, 0.1]\n",
      "Index:  [4500]\n",
      "Belief table entry  [4500] :  [[ 0.3  0.4  0.2  0.1  0.   0. ]]\n",
      "\n",
      "Belief state:  [0.6, 0.3, 0.1, 0.0]\n",
      "Index:  [8360]\n",
      "Belief table entry  [8360] :  [[ 0.6  0.3  0.1  0.   0.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class Maze:\n",
    "\n",
    "    from code import __init__, reset, observe , update_belief_state, choose_next_action, get_next_state\n",
    "    \n",
    "    def get_belief_table_index(self):\n",
    "\n",
    "        self.belief_table_index = np.where((self.belief_table[:, 0] == self.belief_state[0])\n",
    "                                           & (self.belief_table[:, 1] == self.belief_state[1])\n",
    "                                           & (self.belief_table[:, 2] == self.belief_state[2])\n",
    "                                           & (self.belief_table[:, 3] == self.belief_state[3]))[0]\n",
    "        \n",
    "        print \"Index: \", self.belief_table_index\n",
    "        \n",
    "experiment_instance = Maze()\n",
    "experiment_instance.belief_state = [0.0, 0.0, 0.0, 0.0]\n",
    "print \"Belief state: \", experiment_instance.belief_state\n",
    "experiment_instance.get_belief_table_index()\n",
    "print \"Belief table entry \", experiment_instance.belief_table_index, \": \", experiment_instance.belief_table[experiment_instance.belief_table_index]\n",
    "\n",
    "experiment_instance.belief_state = [0.3, 0.4, 0.2, 0.1]\n",
    "print \"\\nBelief state: \", experiment_instance.belief_state\n",
    "experiment_instance.get_belief_table_index()\n",
    "print \"Belief table entry \", experiment_instance.belief_table_index, \": \", experiment_instance.belief_table[experiment_instance.belief_table_index]\n",
    "\n",
    "experiment_instance.belief_state = [0.6, 0.3, 0.1, 0.0]\n",
    "print \"\\nBelief state: \", experiment_instance.belief_state\n",
    "experiment_instance.get_belief_table_index()\n",
    "print \"Belief table entry \", experiment_instance.belief_table_index, \": \", experiment_instance.belief_table[experiment_instance.belief_table_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.belief_table_index = np.where((self.belief_table[:, 0] == self.belief_state[0])\n",
    "                                           & (self.belief_table[:, 1] == self.belief_state[1])\n",
    "                                           & (self.belief_table[:, 2] == self.belief_state[2])\n",
    "                                           & (self.belief_table[:, 3] == self.belief_state[3]))[0]\n",
    "\n",
    "This function consists of just this line.\n",
    "\n",
    "We use the 'where' function in the numpy library. We get the index where all four values in the belief state match the first four values of the entry in the belief table exactly.\n",
    "\n",
    "It does this by finding the entry in the belief table where the first element of the entry matches the first element of the belief state, the second element of the entry matches the second element of the belief state, the third element of the entry matches the third element of the belief state, and the fourth element of the entry matches the fourth element of the belief state.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code will get the index of the belief table corresponding to the belief states: [0.0, 0.0, 0.0, 0.0], [0.3, 0.4, 0.2, 0.1], and [0.6, 0.3, 0.1, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the belief table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we update the Q-values after taking an action.\n",
    "\n",
    "Here's a reminder of the Q-learning algorithm we learnt in the second tutorial:\n",
    "\n",
    "$$Q(s_{t}, a_{t}) \\gets Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma \\cdot maxQ(s_{t+1}, a) - Q(s_{t}, a_{t}))$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor.\n",
    "\n",
    "The above is taken from Sutton and Barto, 1998, pp.148, which is a good resource on reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class Maze:\n",
    "    \n",
    "    from code import __init__, reset, observe, update_belief_state, choose_next_action, get_next_state, get_belief_table_index\n",
    "    \n",
    "    # get highest Q-value in belief state\n",
    "    def calculate_max_q_value(self):\n",
    "\n",
    "        # get the Q-values of the two actions\n",
    "        q_value_a0 = self.belief_table[self.belief_table_index, 4]\n",
    "\n",
    "        q_value_a1 = self.belief_table[self.belief_table_index, 5]\n",
    "\n",
    "        # return the highest\n",
    "        if q_value_a0 > q_value_a1:\n",
    "            return q_value_a0\n",
    "        else:\n",
    "            return q_value_a1\n",
    "\n",
    "    def update_belief_table(self, alpha, gamma):\n",
    "\n",
    "        # get correct element index for the select action\n",
    "        if self.action_number == 0:\n",
    "            location = 4\n",
    "        else:\n",
    "            location = 5\n",
    "\n",
    "        max_q_value = self.calculate_max_q_value()\n",
    "\n",
    "        previous_q = self.belief_table[self.previous_belief_table_index, location]\n",
    "\n",
    "        print \"Real state: \", self.real_state\n",
    "        print \"Action number: \", self.action_number\n",
    "        print \"Max Q: \", max_q_value\n",
    "        print \"Reward: \", self.reward\n",
    "\n",
    "        print \"Before update: \", self.belief_table[self.previous_belief_table_index]\n",
    "\n",
    "        # update Q-value\n",
    "        self.belief_table[self.previous_belief_table_index, location] = previous_q + alpha * (self.reward + gamma * max_q_value - previous_q)\n",
    "\n",
    "        # round to 1 decimal place\n",
    "        self.belief_table[self.previous_belief_table_index, location] = round(self.belief_table[self.previous_belief_table_index, location], 1)\n",
    "\n",
    "        print \"After update: \", self.belief_table[self.previous_belief_table_index]\n",
    "        \n",
    "experiment_instance = Maze()\n",
    "experiment_instance.real_state = 1\n",
    "experiment_instance.action_number = 1\n",
    "experiment_instance.reward = 90\n",
    "experiment_instance.previous_belief_table_index = 100\n",
    "experiment_instance.belief_table_index = 200\n",
    "experiment_instance.belief_table[100] = [0.3, 0.3, 0.2, 0.2, 10, 20]\n",
    "experiment_instance.belief_table[200] = [0.2, 0.2, 0.1, 0.5, 40, 30]\n",
    "\n",
    "experiment_instance.update_belief_table(0.1, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code may require some horizontal scrolling, depending on your monitor size.\n",
    "\n",
    "We will be doing this function slightly differently to the second tutorial. We will be moving to the next state, performing a new observation and updating the belief state before updating the Q-value for the previous belief state and action chosen. This is so we are able to calculate the the max Q-value in the next belief state accurately, as we don't know what the next belief state is until we actually make the move and make another observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a function to calculate the max Q-value in the state.\n",
    "\n",
    "    # get highest Q-value in belief state\n",
    "    def calculate_max_q_value(self):\n",
    "\n",
    "        # get the Q-values of the two actions\n",
    "        q_value_a0 = self.belief_table[self.belief_table_index, 4]\n",
    "\n",
    "        q_value_a1 = self.belief_table[self.belief_table_index, 5]\n",
    "\n",
    "        # return the highest\n",
    "        if q_value_a0 > q_value_a1:\n",
    "            return q_value_a0\n",
    "        else:\n",
    "            return q_value_a1\n",
    "\n",
    "This function is simple, it gets the values for A0 and A1 in the current state, compares them and returns the highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def update_belief_table(self, alpha, gamma):\n",
    "    \n",
    "Now we update the Q-value in the belief table. This function takes in alpha and gamma as parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if self.action_number == 0:\n",
    "            location = 4\n",
    "        else:\n",
    "            location = 5\n",
    "\n",
    "We get the location of the Q-value for the chosen action in the belief table. Entries are in the format: [Probability in S0, Probability in S1, Probability in S2, Probability in S3, Q-value for A0, Q-value for A1], so if A0 has been chosen, the location is set to 4, otherwise it is set to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    max_q_value = self.calculate_max_q_value()\n",
    "    \n",
    "We calculate the maximum Q-value using the previous function discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    previous_q = self.belief_table[self.previous_belief_table_index, location]\n",
    "\n",
    "We get the Q-value for the previous belief state and the action chosen - the Q-value we will be updating.\n",
    "\n",
    "previous_belief_table_index is the the index of belief_table that matches the previous belief_state. We assign and update this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.belief_table[self.previous_belief_table_index, location] = previous_q + alpha * (self.reward + gamma * max_q_value - previous_q)\n",
    "    \n",
    "We update the Q-value using the algorithm we learnt in the second tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.belief_table[self.previous_belief_table_index, location] = round(self.belief_table[self.previous_belief_table_index, location], 1)\n",
    "    \n",
    "We round this updated Q-value to 1 decimal place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code. We insert some arbitrary values so you can see how the update works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code we will use to run the experiment. It is quite long, but straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real state update:  1  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1  0.   0. ]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -1.   0. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.7  0.1  0.1  0.   0. ]]\n",
      "After update:  [[ 0.1  0.7  0.1  0.1 -3.   0. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -1.   0. ]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -1.  -3. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -1.  -3. ]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -1.  -5.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-1.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -1.  -5.7]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -1.  -8.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -1.  -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -1.9 -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  0.   0. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  0.   9. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.7  0.1  0.1 -3.   0. ]]\n",
      "After update:  [[ 0.1  0.7  0.1  0.1 -3.  -3. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-1.9]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -1.9 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -2.9 -8.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -2.9 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -5.6 -8.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-5.6]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  0.   0. ]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  0.  -3.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-3.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -5.6 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -6.3 -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 9.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.1  0.7  0.1  0.1 -3.  -3. ]]\n",
      "After update:  [[ 0.1  0.7  0.1  0.1 -3.   7. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 7.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -6.3 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -6.1 -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 9.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.1  0.7  0.1  0.1 -3.   7. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -3.   16. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 16.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -6.1 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -5.2 -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 9.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -3.   16. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -3.   24.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 24.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -5.2 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -3.8 -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -3.   24.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -3.   30.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 30.7]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -3.8 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1 -2.  -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -3.   30.7]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -3.   36.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  0.   9. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  0.   5.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 36.5]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1 -2.  -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1  0.1 -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -3.   36.5]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -3.   42.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 42.3]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1  0.1 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1  2.5 -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -3.   42.3]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -3.   47.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 47.5]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1  2.5 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1  5.1 -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 47.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -3.   47.5]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -3.   55.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 55.5]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1  5.1 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1  5.1 -5.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 55.5]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1  5.1 -5.9]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1  8.  -5.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -3.   55.5]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -3.   59. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 59.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1  8.  -5.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  10.9  -5.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -3.   59. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -5.7  59. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 59.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  10.9  -5.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  13.5  -5.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -5.7  59. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  59. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  0.   5.1]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  0.   1.6]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  59. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  50.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 50.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  0.  -3.4]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  3.  -3.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 13.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  50.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  55.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 55.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  13.5  -5.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  15.6  -5.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 1.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  55.2]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  58.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 1.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  58.8]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  50. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 50.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  15.6  -5.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  17.   -5.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 3.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  50. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  54.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 54.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  17.   -5.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  18.6  -5.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 54.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  54.2]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  62.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 62.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  62.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  57.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 57.9]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  3.  -3.4]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  6.3 -3.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 18.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  57.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  62.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 1.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  18.6  -5.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  18.6  -8.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 62.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  18.6  -8.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  20.7  -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 1.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  62.6]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  65.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 6.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  20.7  -8.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  18.1  -8.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 6.3]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  6.3 -3.4]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  3.2 -3.4]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 18.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  3.2 -3.4]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  3.3 -3.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 3.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  18.1  -8.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  13.6  -8.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  3.3 -3.4]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  7.2 -3.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 1.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  65.5]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  68.1]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 7.2]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  0.   1.6]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  0.  -1. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 13.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  13.6  -8.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  12.3  -8.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 12.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  12.3  -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1  9.1 -8.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 9.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1  9.1 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1  7.9 -8.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 68.1]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1  7.9 -8.2]]\n",
      "After update:  [[ 0.7  0.1  0.1  0.1  9.6 -8.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 9.6]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  7.2 -3.4]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  7.2 -5.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 68.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.7  0.1  0.1  0.1  9.6 -8.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  13.1  -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  68.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  70.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 70.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  13.1  -8.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  16.4  -8.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  70.3]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.1  72.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 72.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  16.4  -8.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  19.5  -8.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 7.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.1  72.3]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -9.7  72.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 7.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  19.5  -8.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  19.5  -9.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 72.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  19.5  -9.8]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  22.3  -9.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -9.7  72.3]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -9.7  74.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  22.3  -9.8]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  22.3 -11.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 7.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  22.3 -11.8]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  19.6 -11.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 7.2]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  7.2 -5.3]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  4.1 -5.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 74.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  0.  -1. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  4.9 -1. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -9.7  74.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -9.7  76. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  19.6 -11.8]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  22.7 -11.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -9.7  76. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -9.7  77.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  22.7 -11.8]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  22.7 -13.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 77.8]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  4.1 -5.3]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  8.9 -5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -9.7  77.8]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -9.7  79.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -9.7  79.4]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -9.7  69.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 69.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  22.7 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  25.  -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -9.7  69.2]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -9.7  71.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 71.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -9.7  71.7]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.   71.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.   71.7]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.   73.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  4.9 -1. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  4.9 -1.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 25.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  25.  -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  23.5 -13.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  23.5 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  24.1 -13.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  24.1 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  26.6 -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 26.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.   73.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.   77.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  4.9 -1.9]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  3.8 -1.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 77.6]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  3.8 -1.9]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  6.6 -1.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 6.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  26.6 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  23.5 -13.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  6.6 -1.9]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  3.7 -1.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 77.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  23.5 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  26.4 -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 3.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.   77.6]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.   79.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 79.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  26.4 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  29.1 -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 3.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.   79.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.   80.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 80.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  29.1 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  31.6 -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 80.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.   80.5]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.   87.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 31.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.   87.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -2.1  87.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  31.6 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  26.2 -13.2]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 3.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -2.1  87.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -2.1  76.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  26.2 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  28.7 -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 76.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -2.1  76.4]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -2.1  83.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 83.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  28.7 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  31.5 -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 3.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -2.1  83.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -2.1  84.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  31.5 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  34.1 -13.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 34.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -2.1  84.8]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -2.2  84.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  34.1 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  36.5 -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -2.2  84.8]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -2.2  86. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 86.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  36.5 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  38.7 -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 3.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -2.2  86. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -2.2  86.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 86.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  38.7 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  40.8 -13.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -2.2  86.7]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.3  86.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 86.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  40.8 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  42.7 -13.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 3.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.3  86.7]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.3  87.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  42.7 -13.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  42.7 -14.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 87.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  42.7 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  44.4 -14.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 3.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.3  87.3]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.3  87.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.3  87.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.3  76.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  44.4 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  45.1 -14.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 76.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.3  76.8]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.3  84.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.3  84.3]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.2  84.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 45.1]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  8.9 -5.3]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  8.6 -5.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  45.1 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  46.3 -14.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 3.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.2  84.3]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.2  85.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 3.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  46.3 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  41.  -14.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 8.6]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  3.7 -1.9]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  1.  -1.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 85.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  41.  -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  42.7 -14.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 1.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.2  85.2]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.2  85.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 85.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  42.7 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  44.3 -14.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 85.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.2  85.8]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.2  93.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 1.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.2  93.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -4.2  80.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 8.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  44.3 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  39.6 -14.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 8.6]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  8.6 -5.3]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  5.4 -5.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  1.  -1.9]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  6.4 -1.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 6.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -4.2  80.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  80.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  39.6 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  41.1 -14.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 6.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  80.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  82.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 41.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  41.1 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  39.3 -14.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 5.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  39.3 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  32.8 -14.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 82.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  32.8 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  35.1 -14.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  82.3]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  83.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 83.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  35.1 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  37.3 -14.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  83.5]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  87.1]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 5.4]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  6.4 -1.9]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  6.4 -4.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 5.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  37.3 -14.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  37.3 -15.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 87.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  5.4 -5.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  10.8  -5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  87.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  90.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 6.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  90.4]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  78.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 78.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  37.3 -15.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  38.9 -15.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 38.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  78.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  83.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 38.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  10.8  -5.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  11.8  -5.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 11.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  38.9 -15.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  33.  -15.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 33.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  83.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  74.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 11.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  33.  -15.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  29.6 -15.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 11.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  11.8  -5.3]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  8.6 -5.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 6.4]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  6.4 -4.3]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  5.3 -4.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 8.6]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  5.3 -4.3]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7  2.5 -4.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 29.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  29.6 -15.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  28.  -15.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 2.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  28.  -15.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  28.   -4.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 74.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  28.   -4.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  30.2  -4.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 2.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  74.4]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  76.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 2.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  30.2  -4.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  26.4  -4.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 8.6]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7  2.5 -4.3]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7 -0.1 -4.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  26.4  -4.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  28.9  -4.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-0.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  76.2]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  77.6]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-0.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  77.6]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  66.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-0.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  28.9  -4.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  25.   -4.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-0.1]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7 -0.1 -4.3]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.7 -0.1  5.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 25.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  8.6 -5.3]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  8.7 -5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  25.   -4.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  25.    5.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 66.8]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  8.7 -5.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  12.2  -5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  66.8]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  69.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 69.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  25.    5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  27.1   5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  69.5]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  72. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 72.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  27.1   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  29.2   5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  72. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  74.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 74.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  29.2   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  31.2   5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  74.2]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  76.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  31.2   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  33.2   5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  76.2]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  78. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 78.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  33.2   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  35.1   5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  78. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  79.6]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 12.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  79.6]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  69.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 69.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  12.2  -5.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  15.5  -5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  69.6]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  72. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  72. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  63. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 63.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  35.1   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  35.6   5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  63. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  66.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 35.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  35.6   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  33.9   5.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  33.9   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  28.8   5.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  28.8   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  26.2   5.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  15.5  -5.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  12.2  -5.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 66.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  26.2   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  27.9   5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  66.1]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  68.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 68.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  12.2  -5.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  15.5  -5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  68.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -6.3  71.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  27.9   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  24.5   5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.1]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.1  0.1  0.1  0.7 -0.1  5.1]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  -0.1  14. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  15.5  -5.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  15.5  -6.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 24.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  24.5   5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  23.    5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 14.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  23.    5.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  23.   14.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 23.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  23.   14.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  21.5  14.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  21.5  14.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  17.6  14.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 17.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  17.6  14.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  16.2  14.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  16.2  14.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  12.8  14.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  12.8  14.9]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  12.8  11.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 71.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  12.8  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  16.2  11.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 16.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -6.3  71.4]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -7.4  71.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 71.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  16.2  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  19.3  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 14.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -7.4  71.4]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -7.4  74.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 74.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  19.3  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  22.3  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -7.4  74.4]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -7.4  77.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 77.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  22.3  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  25.2  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 77.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -7.4  77.2]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -7.4  84.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  25.2  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  28.5  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 84.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -7.4  84.7]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -7.4  92. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 15.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  28.5  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  25.9  11.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 14.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  15.5  -6.5]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  12.1  -6.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 92.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  25.9  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  29.7  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 14.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -7.4  92. ]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -7.4  92.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 92.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  29.7  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  33.2  11.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 12.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -7.4  92.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.7  92.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 92.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  33.2  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  36.3  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 14.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.7  92.9]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.7  93.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 93.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  36.3  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  39.2  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 14.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.7  93.7]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  -8.7  94.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 39.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  39.2  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  37.4  11.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 12.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  37.4  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  31.6  11.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 94.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  31.6  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  35.   11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 94.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  -8.7  94.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  101.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 101.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  35.   11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  38.6  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 38.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  101.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  103.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 12.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  38.6  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  34.7  11.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 12.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  12.1  -6.5]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  8.9 -6.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  34.7  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  38.5  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 14.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  103.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  103.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 38.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  38.5  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  36.7  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 14.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  36.7  11.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  36.7  20.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  36.7  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  40.3  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 14.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  103.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  103.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 40.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  40.3  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  38.5  20.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 14.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  38.5  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  32.8  20.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 14.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  32.8  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  29.6  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 103.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  -0.1  14. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  -0.1  29.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  29.6  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  33.9  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 103.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  103.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  110. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 110.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  33.9  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  38.3  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  110. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  108.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 108.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  38.3  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  42.2  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  108.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  109.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  42.2  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  45.7  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  109.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  109.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 29.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  45.7  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  42.5  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  -0.1  29.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  -0.1  36.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 36.5]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  8.9 -6.5]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  9.9 -6.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 109.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  -0.1  36.5]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  -0.1  50.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 50.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  42.5  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  41.3  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 9.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  -0.1  50.6]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  -0.1  55.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 9.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  41.3  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  37.   20.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 37.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  9.9 -6.5]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  8.9 -6.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  37.   20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  41.1  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 55.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  109.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  112.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 8.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  41.1  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  41.1  16.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 55.3]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  8.9 -6.5]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  11.4  -6.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 55.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  -0.1  55.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  -0.1  63.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 11.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  41.1  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  36.9  16.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 112.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  11.4  -6.5]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  16.2  -6.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 16.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  36.9  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  33.5  16.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 16.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  16.2  -6.5]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  12.9  -6.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 12.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  33.5  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  30.2  16.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 12.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  12.9  -6.5]]\n",
      "After update:  [[ 0.1  0.1  0.7  0.1  9.6 -6.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  30.2  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  35.2  16.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  112.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  115. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 115.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.7  0.1  9.6 -6.5]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  16.8  -6.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  115. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  117.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 63.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  16.8  -6.5]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  19.2  -6.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  -0.1  63.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  -0.1  70.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 19.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  117.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  104.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  -0.1  70.9]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  70.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  104.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  108.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 108.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  35.2  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  39.4  16.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  108.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  112.4]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 19.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  19.2  -6.5]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  19.2  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  39.4  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  43.5  16.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  112.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  115.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  115.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  106.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 106.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  43.5  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  46.7  16.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 106.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  106.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  113.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  46.7  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  50.1  16.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  113.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  117.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 117.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  19.2  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  25.6  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  117.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  120.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 120.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  50.1  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  53.7  16.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 120.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  120.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  126.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  126.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  113.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  53.7  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  56.4  16.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 56.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  113.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  115.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 115.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  56.4  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  59.   16.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  115.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  118.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 59.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  70.9]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  65.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 118.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  59.   16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  61.6  16.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  118.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  120.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 120.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  61.6  16.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  61.6  21.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 61.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  25.6  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  27.   -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 27.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  61.6  21.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  61.6  30.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 120.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  61.6  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.1  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 27.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  120.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  119.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.1  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  66.3  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  119.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  122.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 27.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  66.3  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  60.8  30.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 27.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  27.   -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  23.5  -7.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 122.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  65.5]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  65.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  60.8  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  63.5  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  122.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  124.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  63.5  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  66.1  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  124.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  126. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 126.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  126. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  120.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 120.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  66.1  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  68.1  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 23.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  120.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  119.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  23.5  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  29.7  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  119.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  118.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 118.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  68.1  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  69.8  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  118.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  118.2]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 29.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  118.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  105.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  69.8  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  70.3  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  105.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  106.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 65.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  70.3  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  67.5  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 67.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  65.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  73.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 73.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  67.5  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  65.6  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 106.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  73.5]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  83.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 29.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  65.6  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  60.4  30.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 83.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  29.7  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  30.4  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 106.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  60.4  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  61.9  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 83.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  106.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  111.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  61.9  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  63.6  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 111.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  111.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -8.7  118.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 83.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  63.6  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  62.9  30.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 83.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  83.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  91. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 118.4]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -8.7  118.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    0.6  118.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 30.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    0.6  118.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    0.6  118. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 30.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  62.9  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  58.   30.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 58.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  30.4  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  29.   -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 29.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  58.   30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  53.5  30.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 29.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  29.   -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  25.4  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  53.5  30.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  53.5  26.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 118.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  53.5  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  56.6  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 56.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    0.6  118. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    0.6  119.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  56.6  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  59.5  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 59.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    0.6  119.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    0.6  121.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  91. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  80.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    0.6  121.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    0.6  112.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 59.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  25.4  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  26.6  -7.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 26.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  59.5  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  52.7  26.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  52.7  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  55.5  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 26.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    0.6  112.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    0.6  112.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  55.5  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  58.   26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 26.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    0.6  112.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    0.6  112.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  58.   26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  60.2  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    0.6  112.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    0.6  116.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 26.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    0.6  116.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    0.6  104.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  60.2  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  61.5  26.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 26.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    0.6  104.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -0.3  104.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  61.5  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  62.7  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -0.3  104.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -0.3  109.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  62.7  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.2  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 109.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -0.3  109.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -0.3  116.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 116.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.2  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  66.1  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 26.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -0.3  116.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   -0.3  115.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 115.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  66.1  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  67.7  26.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   -0.3  115.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  115.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 26.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  67.7  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  62.1  26.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 26.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  26.6  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  23.1  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 115.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  62.1  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.1  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  115.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  119.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  23.1  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  29.4  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  119.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  123. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.1  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  66.5  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  123. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  126.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  66.5  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  65.3  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 80.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  80.9]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  88.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 88.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  65.3  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.8  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  88.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  90.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 90.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.8  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.6  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 90.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  90.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  98. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 64.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.6  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  62.3  26.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 29.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  62.3  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  55.4  26.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 29.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  126.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  112.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 112.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  98. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  94.2]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 29.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  94.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7   7.3  84.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 112.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  112.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  107.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.7   7.3  84.1]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  84.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 84.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  84.1]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  91.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 55.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  91.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  83.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 55.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  55.4  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  53.3  26.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 29.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  53.3  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  47.3  26.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 29.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  107.6]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1   3.2  96.2]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 29.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  83.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  74.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 47.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  47.3  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  45.4  26.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 29.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  45.4  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  40.2  26.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 96.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  29.4  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  33.2  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 96.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1   3.2  96.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  103.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 33.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  33.2  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  31.5  -7.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 103.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  31.5  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  33.6  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  40.2  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  43.4  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 74.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  103.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  107.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  43.4  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  46.7  26.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 33.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  107.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  108.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 46.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  46.7  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  44.8  26.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 33.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  44.8  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  40.   26.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 33.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  40.   26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  37.7  26.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 33.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  33.6  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  29.9  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 108.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  37.7  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  37.7  29.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 108.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  37.7  29.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  41.6  29.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 74.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  108.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  112.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  41.6  29.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  45.5  29.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 112.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  112.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  119.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  45.5  29.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  49.5  29.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 74.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  119.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    3.2  122.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 29.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  29.9  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  28.3  -7.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 74.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  28.3  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  28.4  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  49.5  29.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  53.4  29.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 53.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    3.2  122.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1    4.2  122.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 28.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  28.4  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  28.4  -7.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 28.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  74.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  74.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 28.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  28.4  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  24.8  -7.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 74.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  24.8  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  27.3  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 27.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  74.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  78.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.6]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.7    0.1    0.1    4.2  122.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   12.6  122.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 78.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   12.6  122.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   12.6  125.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 53.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  53.4  29.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  51.3  29.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 27.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  51.3  29.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  45.4  29.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 45.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   12.6  125.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   12.6  113.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  45.4  29.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  49.   29.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 78.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   12.6  113.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   12.6  117.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 49.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  49.   29.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  49.   27.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 117.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  49.   27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  52.5  27.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 78.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   12.6  117.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   12.6  121.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 78.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   12.6  121.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   12.6  112.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 27.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  52.5  27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  48.4  27.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 27.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  27.3  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  23.8  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  48.4  27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  51.5  27.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 112.3]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   12.6  112.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  112.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  51.5  27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  54.3  27.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 112.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  112.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  119.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  23.8  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  29.9  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  119.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  118.6]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 29.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  118.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  106.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 29.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  54.3  27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  50.3  27.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 29.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  29.9  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  26.3  -7.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 106.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  26.3  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  31.2  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 106.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  106.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  113. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 113.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  113. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  107.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 31.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  107.7]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  17.3  96.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 96.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  50.3  27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  52.   27.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 96.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  17.3  96.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  103.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 31.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  103.5]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  17.3  92.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 78.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  52.   27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  52.1  27.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 78.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  78.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  85.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 52.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  52.1  27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  50.1  27.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 31.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  50.1  27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  44.6  27.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 31.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  44.6  27.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  44.6  24.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 92.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  31.2  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  34.5  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 85.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  17.3  92.6]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  17.3  99.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 99.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  44.6  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  47.1  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 85.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  17.3  99.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  105.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  47.1  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  49.8  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 85.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  105.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  110.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 85.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  34.5  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  36.9  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 49.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  85.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  90.2]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 36.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  110.5]]\n",
      "After update:  [[  0.1   0.7   0.1   0.1  17.3  99.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 99.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  49.8  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  51.8  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 90.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.7   0.1   0.1  17.3  99.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  105.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 36.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  90.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  81.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  51.8  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  54.1  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 36.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  105.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  107.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  54.1  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  56.3  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 56.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  107.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  109.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  56.3  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  58.5  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 109.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  109.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  116.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 116.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  58.5  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  61.   24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 81.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  116.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  120.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 36.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  61.   24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  56.9  24.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 36.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  36.9  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  33.2  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 120.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  56.9  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  59.9  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 120.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  120.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  127.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 127.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  33.2  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  39.   -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 59.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  127.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  128.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 128.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  39.   -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  44.4  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 81.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  128.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  130.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 59.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  44.4  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  43.8  -7.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 43.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  59.9  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  54.4  24.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 43.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  54.4  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  51.5  24.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 43.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  43.8  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  39.9  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 51.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  51.5  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  49.5  24.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 130.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  49.5  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  52.   24.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 81.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  39.9  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  41.4  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 52.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  81.1]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  86.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  52.   24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  56.3  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 86.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  130.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  133.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 41.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  56.3  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  53.   24.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 41.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  41.4  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  37.6  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  53.   24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  49.7  24.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 133.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  37.6  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  41.5  -7.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  41.5  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  47.   -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 47.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  133.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  133.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 47.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  133.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  120.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 120.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  49.7  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  53.4  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 120.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  120.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  127.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 127.1]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  127.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.3  121.6]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 47.]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.3  121.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.3  121.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 86.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  47.   -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  46.2  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 121.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  53.4  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  56.8  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 56.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.3  121.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.3  123. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  56.8  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  60.   24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 60.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.3  123. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.3  124.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  60.   24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  63.   24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 86.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.3  124.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.3  127.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 127.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  63.   24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  65.9  24.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 86.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.3  127.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.3  131. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 46.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  65.9  24.2]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  65.9  22.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 131.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  65.9  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  68.8  22.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 86.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.3  131. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.3  133.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  68.8  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  71.6  22.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 133.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.3  133.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.3  140.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 140.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  46.2  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  51.8  -7.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 71.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.3  140.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.3  140.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 140.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  71.6  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  74.7  22.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 51.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.3  140.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  140.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 51.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  140.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  127.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 51.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  86.1]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  78.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 127.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  74.7  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  76.5  22.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 78.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  127.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  130.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  78.6]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  73.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  130.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  120.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 120.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  76.5  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  77.5  22.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  120.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  123.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 51.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  77.5  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  72.9  22.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 51.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  51.8  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  47.8  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 47.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  72.9  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  68.4  22.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 47.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  47.8  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  43.8  -7.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  68.4  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  70.4  22.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  123.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  125.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 43.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  125.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  113.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  70.4  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  71.4  22.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  113.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  117.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 117.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  71.4  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  72.6  22.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  117.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  120.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 43.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  72.6  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  67.8  22.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  43.8  -7.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  43.8   8.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 67.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  67.8  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  65.4  22.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 43.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  65.4  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  59.4  22.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 120.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  59.4  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  62.1  22.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  120.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  123.2]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 123.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  123.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   17.6  117.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 117.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  62.1  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.3  22.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 43.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   17.6  117.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   16.3  117.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 117.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  43.8   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  47.8   8.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 47.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   16.3  117.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   16.3  118.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 118.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  47.8   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  51.5   8.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   16.3  118.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   16.3  121.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 51.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.3  22.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.3  21.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 64.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  73.9]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  68.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 121.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.3  21.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  66.6  21.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 68.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   16.3  121.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   16.3  124.1]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 66.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  68.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  64.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  66.6  21.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  68.9  21.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 64.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   16.3  124.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   16.3  125.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 51.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  68.9  21.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  65.1  21.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 65.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  51.5   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  48.6   8.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 48.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  65.1  21.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  61.5  21.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 48.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  48.6   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  44.6   8.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 125.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  44.6   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  49.2   8.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 49.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   16.3  125.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  125.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 64.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  49.2   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  48.4   8.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 64.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  64.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  71.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 125.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  61.5  21.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.4  21.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 71.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  125.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  128. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 71.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  48.4   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  48.3   8.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 71.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  71.9]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  79.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 48.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.4  21.4]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.4  20.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 128.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.4  20.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  67.2  20.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 128.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  128. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  134.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 134.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  67.2  20.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  70.2  20.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 48.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  134.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  133.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 79.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  70.2  20.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  68.5  20.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 79.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  79.5]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  86.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  68.5  20.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  71.4  20.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 86.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  133.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  136.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 136.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  71.4  20.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  74.2  20.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 86.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  136.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  138.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 138.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  74.2  20.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  76.9  20.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 76.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  138.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  140. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 140.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  76.9  20.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  79.4  20.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 86.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  140. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  142. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 48.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  79.4  20.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  79.4  19. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 48.3]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  142. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  128.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 128.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  79.4  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  80.8  19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 80.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  128.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  131.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 131.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  80.8  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  82.2  19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 86.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  131.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  134.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 48.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  48.3   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  48.3   8.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 86.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  82.2  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  79.9  19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 134.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  86.9]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  12.3  97.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 134.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  79.9  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  81.6  19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 97.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  134.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  137.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 48.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  48.3   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  46.3   8.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 97.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  46.3   8.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  46.3  24.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 137.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  12.3  97.9]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  21.1  97.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 97.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  137.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  140.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 140.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  81.6  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  83.7  19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 140.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  140.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  146.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 146.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  83.7  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  86.1  19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 86.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  146.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  148. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 148.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  86.1  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  88.3  19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 88.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  148. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  149.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 149.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  88.3  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  90.4  19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 97.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  149.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  151.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 46.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  90.4  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  84.1  19. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 46.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  46.3  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  42.4  24.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 151.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  84.1  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  86.8  19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 42.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  151.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  148.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 97.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  86.8  19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  85.   19. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 97.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  21.1  97.9]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   21.1  104.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 42.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  85.   19. ]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  85.   17.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 148.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  85.   17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  87.4  17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 104.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  148.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  151. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 42.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  87.4  17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  81.1  17.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 42.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  42.4  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  38.6  24.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 151.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  81.1  17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  84.1  17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 151.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  151. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  157. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 157.]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   21.1  104.9]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   30.6  104.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 104.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  157. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  158.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 38.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  158.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  142.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  38.6  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  42.1  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 84.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   30.6  104.9]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   30.6  110.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 142.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  84.1  17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  86.1  17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 110.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  142.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  146.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 146.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  86.1  17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  88.2  17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 146.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  146.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  152.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 152.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  88.2  17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  90.6  17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 110.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  152.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  155.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 155.1]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  155.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  149. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 149.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  90.6  17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  92.5  17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 110.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  149. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  151.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 151.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  92.5  17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  94.4  17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 110.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  151.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  154.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 154.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  94.4  17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  96.3  17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 110.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  154.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  156.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 156.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   30.6  110.1]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   30.6  108.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 156.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  42.1  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  49.4  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 156.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  156.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   15.6  162.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 162.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  96.3  17.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  98.7  17.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 98.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   15.6  162.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.9  162.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 49.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   30.6  108.6]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  30.6  98.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 162.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.9  162.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.9  156.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 156.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  98.7  17.5]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  100.4   17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 100.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.9  156.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.9  157.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 157.9]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.7    0.1    0.1    0.1  100.4   17.5]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  102.    17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 49.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.9  157.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.9  155.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 155.1]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.7    0.1    0.1    0.1  102.    17.5]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  103.2   17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 103.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.9  155.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.9  156.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 156.8]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.7    0.1    0.1    0.1  103.2   17.5]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  104.4   17.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 98.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.9  156.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   18.9  158. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 98.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.7    0.1    0.1    0.1  104.4   17.5]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  104.4   20.6]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 158.]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   18.9  158. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  158. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 104.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  158. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  159.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.7]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.7    0.1    0.1    0.1  104.4   20.6]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  100.9   20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 49.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  30.6  98.7]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   30.6  101.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 159.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  159.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  153.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 153.4]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.7    0.1    0.1    0.1  100.9   20.6]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  102.1   20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 101.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  153.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  155.2]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 49.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   30.6  101.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  30.6  92.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 155.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  30.6  92.6]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  30.6  92.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 49.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  30.6  92.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  30.6  84.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 155.2]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.7    0.1    0.1    0.1  102.1   20.6]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  103.3   20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 84.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  155.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  155.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 49.4]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.7    0.1    0.1    0.1  103.3   20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  95.9  20.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 49.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  49.4  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  45.4  24.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 155.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  95.9  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  97.7  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 97.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  155.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  156.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 97.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  97.7  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  94.7  20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 84.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  94.7  20.6]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  94.7  34.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 45.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  156.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  141.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 141.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  141.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  135.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 135.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  30.6  84.5]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  30.6  83.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 83.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  94.7  34.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  90.9  34.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 83.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  30.6  83.9]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  30.6  91.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 45.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  90.9  34.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  90.9  31.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 135.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  90.9  31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  91.7  31.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  135.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  138.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 45.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  91.7  31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  85.2  31.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 45.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  45.4  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  41.5  24.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 91.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  138.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  129. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 129.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  85.2  31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  86.   31.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  129. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  132.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 91.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  86.   31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  83.7  31.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  30.6  91.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  30.6  98.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  83.7  31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  82.2  31.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 98.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  30.6  98.4]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   30.6  105.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 82.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  132.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  122.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  82.2  31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  82.8  31.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  122.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  127.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 127.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  41.5  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  46.6  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  127.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  132.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 132.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  82.8  31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  84.1  31.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 46.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  132.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  132. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 46.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  84.1  31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  78.4  31.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 46.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  46.6  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  42.7  24.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 132.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  78.4  31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  80.1  31.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  132. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  136.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 42.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  80.1  31.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  80.1  28.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 42.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  80.1  28.8]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  80.1  26.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 42.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  80.1  26.3]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  80.1  24.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 136.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  80.1  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  82.   24.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  136.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  140. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 42.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  140. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  126.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 42.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  82.   24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  76.2  24.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 126.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  42.7  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  45.5  24.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 126.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  126.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  120.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 45.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  120.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  109.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 45.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  76.2  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  71.2  24.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  45.5  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  46.4  24.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 46.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  71.2  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  66.8  24.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 46.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  46.4  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  42.5  24.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  66.8  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  67.9  24.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  109.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  116. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 116.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  42.5  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  46.5  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 116.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  116. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  122.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 46.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  67.9  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  63.8  24.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 63.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  46.5  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  44.   24.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  63.8  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  64.9  24.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   30.6  105.4]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   30.6  112.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  64.9  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  66.4  24.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 112.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   30.6  112.3]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   30.6  119.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  66.4  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  68.6  24.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 68.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  122.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  124.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  68.6  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  70.7  24.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 44.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  124.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  124.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  70.7  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  72.6  24.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 119.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  124.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.6  130.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 72.6]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.6  130.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   30.5  130.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 72.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  72.6  24.1]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  72.6  36.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  72.6  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  73.9  36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 130.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   30.6  119.1]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   30.6  126.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  73.9  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  76.   36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 126.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   30.5  130.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   30.5  136.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 136.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  76.   36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  78.4  36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 126.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   30.5  136.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   30.5  142.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 142.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  44.   24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  50.   24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 142.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   30.5  142.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   30.5  148.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 126.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  78.4  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  79.7  36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 126.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   30.6  126.7]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   30.6  133.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 79.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  79.7  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  77.1  36.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  77.1  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  77.   36.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 148.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  77.   36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  80.2  36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   30.5  148.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   30.5  153.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 153.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  80.2  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  83.4  36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   30.5  153.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   30.5  157.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 157.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  83.4  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  86.7  36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   30.5  157.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   30.5  161.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 161.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  86.7  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  90.   36.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 50.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   30.5  161.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  161.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 161.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  50.   24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  56.9  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  161.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  165. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 165.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  90.   36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  93.2  36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 56.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  165. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  162.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 162.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  93.2  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  95.8  36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  162.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  165.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 165.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  56.9  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  63.5  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  165.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  163. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 163.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  63.5  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  69.2  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  163. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  166.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 95.8]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   30.6  133.2]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   34.2  133.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 166.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  95.8  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  96.5  36.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 69.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  69.2  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  66.8  24.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 66.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  66.8  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  62.5  24.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 62.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   34.2  133.2]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   34.2  121.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 166.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  62.5  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  68.6  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 96.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  166.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  166.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 121.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   34.2  121.9]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   34.2  116.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 166.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  96.5  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  99.2  36.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 116.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  166.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  168.2]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 99.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  168.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  156.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 68.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   34.2  116.5]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   34.2  107.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 156.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  68.6  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  73.2  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 107.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  156.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  158.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  158.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  145.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  99.2  36.5]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  99.2  35.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 145.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  73.2  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  76.5  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 107.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  145.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  148.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 148.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  99.2  35.7]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  100.2   35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 107.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  148.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  151.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 100.2]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.7    0.1    0.1    0.1  100.2   35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  97.2  35.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  97.2  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  90.6  35.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  151.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  139.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  139.1]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  128.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 128.3]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   34.2  107.3]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   40.   107.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 128.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  128.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  134.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  134.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  124.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  124.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  115. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 115.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  90.6  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  89.7  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  115. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  118.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 118.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  89.7  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  89.2  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  118.6]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  121.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  89.2  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  87.9  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 107.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   40.   107.3]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   40.   114.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 121.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  87.9  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  87.9  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 114.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  121.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  127.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 127.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  87.9  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  88.3  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 114.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  127.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   28.4  133.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  88.3  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  89.1  35.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   28.4  133.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  133.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  89.1  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  89.8  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 114.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  133.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  138. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  89.8  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  87.   35.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  87.   35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  81.4  35.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 138.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  81.4  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  83.3  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 114.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  138. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  142.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  83.3  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  80.1  35.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  76.5  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  72.   24.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 142.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  80.1  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  82.5  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 114.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  142.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  146.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 146.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  82.5  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  84.9  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 114.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  146.2]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  149.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 149.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  84.9  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  87.4  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 87.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  149.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  150.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 72.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   40.   114.2]]\n",
      "After update:  [[   0.1    0.1    0.1    0.7   40.   105.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 150.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  87.4  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  89.7  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 89.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  150.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  151.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 151.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  72.   24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  75.9  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 89.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  151.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  152.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 75.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  89.7  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  85.8  35.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 105.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  75.9  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  73.8  24.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.1    0.1    0.7   40.   105.5]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  40.   97.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  40.   97.9]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  40.   91. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 73.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  85.8  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  82.1  35.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 91.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  73.8  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  70.7  24.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 152.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  82.1  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  85.1  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 152.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  152.8]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  158.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 158.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  85.1  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  88.3  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 158.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  158.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  164.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 91.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  164.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  152.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 152.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  88.3  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  90.7  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  152.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  153.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 153.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  90.7  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  92.9  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  153.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  154.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 154.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  92.9  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  95.   35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  154.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  153.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 153.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  70.7  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  74.9  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  153.5]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  154.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 154.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  95.   35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  96.9  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 74.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  154.4]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   33.2  154. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 154.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  96.9  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  98.5  35.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 74.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   33.2  154. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   32.9  154. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  74.9  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  74.3  24.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 74.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  98.5  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  91.6  35.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 154.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  91.6  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  93.8  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   32.9  154. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   32.9  154.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 154.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  93.8  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  95.8  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   32.9  154.9]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   32.9  155.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 155.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  95.8  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  97.7  35.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 74.3]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   32.9  155.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   32.6  155.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 74.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  40.   91. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  40.   84.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  74.3  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  72.7  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 84.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  40.   84.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  40.   92.1]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 72.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  40.   92.1]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  40.   85.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 155.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  97.7  35.7]]\n",
      "After update:  [[  0.7   0.1   0.1   0.1  99.4  35.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 85.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   32.6  155.7]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   32.6  156. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 156.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.7   0.1  72.7  24.3]]\n",
      "After update:  [[  0.1   0.1   0.7   0.1  76.9  24.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 85.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   32.6  156. ]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   32.6  156.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 76.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.7  40.   85.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.7  40.   80.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 156.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.7   0.1   0.1   0.1  99.4  35.7]]\n",
      "After update:  [[   0.7    0.1    0.1    0.1  101.    35.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 76.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.7    0.1    0.1   32.6  156.3]]\n",
      "After update:  [[   0.1    0.7    0.1    0.1   32.5  156.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Number of fails:\n",
      "221\n",
      "\n",
      "Number of rewards:\n",
      "279\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "class Maze:\n",
    "    \n",
    "    from code import __init__, reset, observe, update_belief_state, choose_next_action, get_next_state, get_belief_table_index, calculate_max_q_value, update_belief_table\n",
    "\n",
    "    def run_experiment(self, initial_state, observation_chance, alpha, gamma, epsilon, num_runs_wanted):\n",
    "\n",
    "        # set current state to initial state wanted\n",
    "        self.real_state = initial_state\n",
    "\n",
    "        # make an initial run\n",
    "\n",
    "        # make an observation\n",
    "        observation = self.observe(observation_chance)\n",
    "\n",
    "        # update belief state with observation\n",
    "        self.update_belief_state(observation, observation_chance)\n",
    "\n",
    "        # get the corresponding index in the belief table\n",
    "        self.get_belief_table_index()\n",
    "\n",
    "        # choose next action\n",
    "        self.choose_next_action(epsilon)\n",
    "\n",
    "        # get the next state\n",
    "        self.get_next_state()\n",
    "\n",
    "        # get the reward for moving from current state to next state\n",
    "        self.reward = self.r[self.real_state][self.next_state]\n",
    "\n",
    "        # update total reward\n",
    "        self.total_reward += self.reward\n",
    "\n",
    "        # move to the next state\n",
    "        self.real_state = self.next_state\n",
    "        self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "        print \"Real state update: \", self.real_state, \" --> \", self.next_state\n",
    "\n",
    "        # variables to keep track\n",
    "        num_rewards_received = 0\n",
    "        num_fails = 0\n",
    "\n",
    "        num_runs = 0\n",
    "\n",
    "        while num_runs < num_runs_wanted:\n",
    "\n",
    "            print \"\"\n",
    "\n",
    "            # make an observation\n",
    "            observation = self.observe(observation_chance)\n",
    "\n",
    "            # update belief state with observation\n",
    "            self.update_belief_state(observation, observation_chance)\n",
    "\n",
    "            # keep track of the previous belief_table_index\n",
    "            self.previous_belief_table_index = self.belief_table_index\n",
    "\n",
    "            # get the corresponding index in the belief table\n",
    "            self.get_belief_table_index()\n",
    "\n",
    "            print \"Observation: \", observation\n",
    "\n",
    "            # update the belief table using Q-learning\n",
    "            self.update_belief_table(alpha, gamma)\n",
    "\n",
    "            # if terminal state reached\n",
    "            if self.real_state == 2 or self.real_state == 3:\n",
    "\n",
    "                if self.real_state == 2:\n",
    "                    num_fails += 1\n",
    "                elif self.real_state == 3:\n",
    "                    num_rewards_received += 1\n",
    "\n",
    "                num_runs += 1\n",
    "\n",
    "                self.expected_value_array = np.append(self.expected_value_array, self.total_reward / (80 * num_runs))\n",
    "\n",
    "                # reset to inital values\n",
    "                self.reset(initial_state)\n",
    "\n",
    "                # make an observation\n",
    "                observation = self.observe(observation_chance)\n",
    "\n",
    "                # update belief state with observation\n",
    "                self.update_belief_state(observation, observation_chance)\n",
    "\n",
    "                print \"Observation: \", observation\n",
    "\n",
    "                # get the corresponding index in the belief table\n",
    "                self.get_belief_table_index()\n",
    "\n",
    "                # keep track of the previous belief_table_index\n",
    "                self.previous_belief_table_index = self.belief_table_index\n",
    "\n",
    "            # choose next action\n",
    "            self.choose_next_action(epsilon)\n",
    "\n",
    "            # get the next state\n",
    "            self.get_next_state()\n",
    "\n",
    "            # get the reward for moving from current state to next state\n",
    "            self.reward = self.r[self.real_state][self.next_state]\n",
    "\n",
    "            # update total reward\n",
    "            self.total_reward += self.reward\n",
    "\n",
    "            print \"Real state update: \", self.real_state, \" --> \", self.next_state\n",
    "\n",
    "            # move to the next state\n",
    "            self.real_state = self.next_state\n",
    "            self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "        print \"\"\n",
    "        print \"Number of fails:\"\n",
    "        print num_fails\n",
    "\n",
    "        print \"\"\n",
    "        print \"Number of rewards:\"\n",
    "        print num_rewards_received\n",
    "        \n",
    "experiment_instance = Maze()\n",
    "experiment_instance.run_experiment(0, 0.65, 0.1, 0.8, 0.1, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code combines all our functions. Let's walk through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def run_experiment(self, initial_state, observation_chance, alpha, gamma, epsilon, num_runs_wanted):\n",
    "    \n",
    "The parameters needed are the initial state you want the rat to start in, alpha, gamma, and the number of runs wanted. A run is completed when a terminal state, S2 or S3, is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self.real_state = initial_state\n",
    "\n",
    "We set the real state to the initial state to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # make an observation\n",
    "    observation = self.observe(observation_chance)\n",
    "\n",
    "    # update belief state with observation\n",
    "    self.update_belief_state(observation, observation_chance)\n",
    "\n",
    "    # get the corresponding index in the belief table\n",
    "    self.get_belief_table_index()\n",
    "\n",
    "    # choose next action\n",
    "    self.choose_next_action(epsilon)\n",
    "\n",
    "    # get the next state\n",
    "    self.get_next_state()\n",
    "\n",
    "    # get the reward for moving from current state to next state\n",
    "    self.reward = self.r[self.real_state][self.next_state]\n",
    "    \n",
    "    # update total reward\n",
    "    self.total_reward += self.reward\n",
    "\n",
    "    # move to the next state\n",
    "    self.real_state = self.next_state\n",
    "    self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "This code makes an initial run.\n",
    "\n",
    "We make an observation using our observation function.\n",
    "\n",
    "We then update our belief state using our observation.\n",
    "\n",
    "We find the corresponding index in the belief table.\n",
    "\n",
    "We choose the next action using our function.\n",
    "\n",
    "We get the next state using our function.\n",
    "\n",
    "We calculate the reward for moving from the initial state to the next state.\n",
    "\n",
    "We add the reward to total_reward.\n",
    "\n",
    "We then set the real state to the next state - we make the move.\n",
    "\n",
    "We reset the belief state when we make the move. We do this because we've moved to a new state, and the rat has not performed an observation yet, so has no indication as to what state it is in.\n",
    "\n",
    "There are a few other ways of changing this, such as taking into account the movement between states, and adjusting beliefs accordingly, but the reset method is the method we will use for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # variables to keep track \n",
    "    num_rewards_received = 0\n",
    "    num_fails = 0\n",
    "\n",
    "    num_runs = 0\n",
    "    \n",
    "We use these to keep track of the number of rewards we have received, the number of times we have reached a terminal state without a reward, and the number of runs we have made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then reach the while loop, which runs for the number of runs wanted. Let's take the code within the while loop part by part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # make an observation\n",
    "    observation = self.observe(observation_chance)\n",
    "\n",
    "    # update belief state with observation\n",
    "    self.update_belief_state(observation, observation_chance)\n",
    "\n",
    "    # keep track of the previous belief_table_index\n",
    "    self.previous_belief_table_index = self.belief_table_index\n",
    "\n",
    "    # get the corresponding index in the belief table\n",
    "    self.get_belief_table_index()\n",
    "\n",
    "    # update the belief table using Q-learning\n",
    "    self.update_belief_table(alpha, gamma)\n",
    "    \n",
    "Once we have moved to our next state, we perform a new observation and update the belief state with this observation.\n",
    "\n",
    "We then store the previously calculated belief_table_index in previous_belief_table_index to keep track of it, before getting the new belief_table_index which corresponds to our new belief state.\n",
    "\n",
    "We then update the belief table using our update_belief_table function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # if terminal state reached\n",
    "    if self.real_state == 2 or self.real_state == 3:\n",
    "\n",
    "        if self.real_state == 2:\n",
    "            num_fails += 1\n",
    "        elif self.real_state == 3:\n",
    "            num_rewards_received += 1\n",
    "\n",
    "        num_runs += 1\n",
    "\n",
    "        self.expected_value_array = np.append(self.expected_value_array, self.total_reward / (80 * num_runs))\n",
    "\n",
    "        # reset to inital values\n",
    "        self.reset(initial_state)\n",
    "\n",
    "        # make an observation\n",
    "        observation = self.observe(observation_chance)\n",
    "\n",
    "        # update belief state with observation\n",
    "        self.update_belief_state(observation, observation_chance)\n",
    "\n",
    "        # get the corresponding index in the belief table\n",
    "        self.get_belief_table_index()\n",
    "\n",
    "        # keep track of the previous belief_table_index\n",
    "        self.previous_belief_table_index = self.belief_table_index\n",
    "\n",
    "The next line of code checks if we are in a terminal state.\n",
    "\n",
    "If we are, and we are in S2, num_fails is incremented by 1, as we have reached a terminal state without receiving a reward.\n",
    "\n",
    "If we are in S3, num_rewards_received is incremented by 1, as we have reached the reward state.\n",
    "\n",
    "num_runs is incremented by 1, as a run has been completed.\n",
    "\n",
    "We then append the expected value, after this run, to our expected value array. We calculate the expected value like this:\n",
    "\n",
    "The expected value is the mean value over all runs. The maximum reward for a run is 80. The maximum total reward that could have been achieved by the rat over all runs is 80 \\* num_runs. We divide the total reward we have achieved by 80 \\* num_runs to get the expected value.\n",
    "\n",
    "We call our reset function, which sets the belief state back to the original belief state, and the real state to the initial state. You may notice that both reset and update_belief_state will change the belief_state to [0.3, 0.3, 0.3, 0.3], making the reset redundant in one case. As discussed previously, update_belief_state can have a different method of changing the belief state, other than resetting it to [0.3, 0.3, 0.3, 0.3], so we reset it beforehand with our 'reset' function.\n",
    "\n",
    "We then do an observation from this initial state, update the belief state, get the corresponding index of the belief state, and use previous_belief_table_index to keep track of this index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # choose next action\n",
    "    self.choose_next_action(epsilon)\n",
    "\n",
    "    # get the next state\n",
    "    self.get_next_state()\n",
    "\n",
    "    # get the reward for moving from current state to next state\n",
    "    self.reward = self.r[self.real_state][self.next_state]\n",
    "    \n",
    "    # update total reward\n",
    "    self.total_reward += self.reward\n",
    "\n",
    "    print \"Real state update: \", self.real_state, \" --> \", self.next_state\n",
    "\n",
    "    # move to the next state\n",
    "    self.real_state = self.next_state\n",
    "    self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "    \n",
    "This final part chooses the next action, gets the correct next state for this chosen action, calculates the reward for moving to this state, adds the reward to total_reward, sets real_state to the next_state (moves the rat to the next state), and resets the belief state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code will run the experiment for 500 runs, starting from S0, with an observation chance of 0.65, an alpha of 0.1, a gamma of 0.8, and an epsilon of 0.1.\n",
    "\n",
    "After running, it will print out all the belief_table entries that have had their Q-values for either A0 or A1 modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will talk through how to get the expected values for different observation values, and plot them on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real state update:  1  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.  0.  0.  0.  0.  0.]]\n",
      "After update:  [[ 1.  0.  0.  0. -1.  0.]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.  1.  0.  0.  0.  0.]]\n",
      "After update:  [[ 0.  1.  0.  0.  0.  9.]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 1.  0.  0.  0. -1.  0.]]\n",
      "After update:  [[ 1.  0.  0.  0. -1. -3.]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 9.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.  0.  0.  0. -1. -3.]]\n",
      "After update:  [[ 1.   0.   0.   0.  -1.2 -3. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.  1.  0.  0.  0.  9.]]\n",
      "After update:  [[ 0.  1.  0.  0. -3.  9.]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 9.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.   0.   0.   0.  -1.2 -3. ]]\n",
      "After update:  [[ 1.   0.   0.   0.  -1.4 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.  1.  0.  0. -3.  9.]]\n",
      "After update:  [[  0.    1.    0.    0.   -3.   17.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 17.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.   0.   0.   0.  -1.4 -3. ]]\n",
      "After update:  [[ 1.   0.   0.   0.  -0.9 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -3.   17.1]]\n",
      "After update:  [[  0.    1.    0.    0.   -3.   24.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 24.4]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.   0.   0.   0.  -0.9 -3. ]]\n",
      "After update:  [[ 1.   0.   0.   0.   0.1 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -3.   24.4]]\n",
      "After update:  [[  0.   1.   0.   0.  -3.  31.]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 31.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.   0.   0.   0.   0.1 -3. ]]\n",
      "After update:  [[ 1.   0.   0.   0.   1.6 -3. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.   1.   0.   0.  -3.  31.]]\n",
      "After update:  [[  0.    1.    0.    0.   -5.7  31. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 31.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.   0.   0.   0.   1.6 -3. ]]\n",
      "After update:  [[ 1.   0.   0.   0.   2.9 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -5.7  31. ]]\n",
      "After update:  [[  0.    1.    0.    0.   -5.7  36.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 36.9]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.   0.   0.   0.   2.9 -3. ]]\n",
      "After update:  [[ 1.   0.   0.   0.   4.6 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -5.7  36.9]]\n",
      "After update:  [[  0.    1.    0.    0.   -5.7  42.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 42.2]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.   0.   0.   0.   4.6 -3. ]]\n",
      "After update:  [[ 1.   0.   0.   0.   6.5 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -5.7  42.2]]\n",
      "After update:  [[  0.    1.    0.    0.   -5.7  47. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 47.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.   0.   0.   0.   6.5 -3. ]]\n",
      "After update:  [[ 1.   0.   0.   0.   8.6 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -5.7  47. ]]\n",
      "After update:  [[  0.    1.    0.    0.   -5.7  51.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 51.3]\n",
      "Reward:  -10\n",
      "Before update:  [[ 1.   0.   0.   0.   8.6 -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   10.8  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -5.7  51.3]]\n",
      "After update:  [[  0.    1.    0.    0.   -5.7  55.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 55.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   10.8  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   13.1  -3. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.   -5.7  55.2]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  55.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 55.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   13.1  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   15.2  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  55.2]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  58.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 58.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   15.2  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   17.4  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  58.7]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  61.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 61.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   17.4  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   19.6  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  61.8]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  64.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 64.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   19.6  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   21.8  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  64.6]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  67.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 67.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   21.8  -3. ]]\n",
      "After update:  [[  1.   0.   0.   0.  24.  -3.]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  67.1]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  69.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 69.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.   0.   0.   0.  24.  -3.]]\n",
      "After update:  [[  1.    0.    0.    0.   26.2  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  69.4]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  71.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 71.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   26.2  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   28.3  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  71.5]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  73.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 73.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   28.3  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   30.3  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  73.3]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  75. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 75.]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   30.3  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   32.3  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  75. ]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  76.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   32.3  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   34.2  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  76.5]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  77.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 77.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   34.2  -3. ]]\n",
      "After update:  [[  1.   0.   0.   0.  36.  -3.]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  77.8]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  79. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 79.]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.   0.   0.   0.  36.  -3.]]\n",
      "After update:  [[  1.    0.    0.    0.   37.7  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  79. ]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  80.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   37.7  -3. ]]\n",
      "After update:  [[  1.    0.    0.    0.   37.7  -5.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 80.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   37.7  -5.7]]\n",
      "After update:  [[  1.    0.    0.    0.   39.3  -5.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  80.1]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  81.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 81.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   39.3  -5.7]]\n",
      "After update:  [[  1.    0.    0.    0.   40.9  -5.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  81.1]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  82. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 82.]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   40.9  -5.7]]\n",
      "After update:  [[  1.    0.    0.    0.   42.4  -5.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  82. ]]\n",
      "After update:  [[  0.    1.    0.    0.   -8.1  82.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 82.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   42.4  -5.7]]\n",
      "After update:  [[  1.    0.    0.    0.   43.8  -5.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.   -8.1  82.8]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  82.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   43.8  -5.7]]\n",
      "After update:  [[  1.    0.    0.    0.   43.8  -8.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 82.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   43.8  -8.1]]\n",
      "After update:  [[  1.    0.    0.    0.   45.   -8.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  82.8]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  83.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 83.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   45.   -8.1]]\n",
      "After update:  [[  1.    0.    0.    0.   46.2  -8.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  83.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  84.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   46.2  -8.1]]\n",
      "After update:  [[  1.    0.    0.    0.   47.3  -8.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  84.2]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  84.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   47.3  -8.1]]\n",
      "After update:  [[  1.    0.    0.    0.   48.4  -8.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  84.8]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  85.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 85.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   48.4  -8.1]]\n",
      "After update:  [[  1.    0.    0.    0.   49.4  -8.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  85.3]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  85.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 85.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   49.4  -8.1]]\n",
      "After update:  [[  1.    0.    0.    0.   50.3  -8.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  85.8]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  86.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 86.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   50.3  -8.1]]\n",
      "After update:  [[  1.    0.    0.    0.   51.2  -8.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  86.2]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  86.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   51.2  -8.1]]\n",
      "After update:  [[  1.    0.    0.    0.   51.2 -10.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 86.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   51.2 -10.3]]\n",
      "After update:  [[  1.    0.    0.    0.   52.  -10.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  86.6]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  86.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   52.  -10.3]]\n",
      "After update:  [[  1.    0.    0.    0.   52.  -12.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 86.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   52.  -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   52.8 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  86.9]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  87.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 87.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   52.8 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   53.5 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  87.2]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  87.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 87.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   53.5 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   54.1 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  87.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  87.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 87.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   54.1 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   54.7 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  87.8]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  88. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 88.]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   54.7 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   55.3 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  88. ]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  88.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 88.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   55.3 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   55.8 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  88.2]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  88.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 88.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   55.8 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   56.3 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  88.4]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  88.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 88.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   56.3 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   56.8 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  88.6]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  88.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 88.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   56.8 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   57.2 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  88.7]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  88.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 88.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   57.2 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   57.6 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  88.8]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  88.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 88.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   57.6 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   58.  -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  88.9]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  89. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   58.  -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   58.3 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  89. ]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  89.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   58.3 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   58.6 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  89.1]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  89.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   58.6 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   58.9 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  89.2]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  89.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   58.9 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   59.2 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  89.3]]\n",
      "After update:  [[  0.    1.    0.    0.  -10.3  89.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   59.2 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   59.4 -12.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -10.3  89.4]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   59.4 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   59.6 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.4]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   59.6 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   59.8 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   59.8 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   60.  -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   60.  -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   60.2 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   60.2 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   60.3 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   60.3 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   60.4 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   60.4 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   60.5 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   60.5 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   60.6 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   60.6 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   60.7 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   60.7 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   60.8 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   60.8 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   60.9 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   60.9 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.  -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.  -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.1 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.1 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -12.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -12.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -12.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -14.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -15.7  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -15.7  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -15.7  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -15.7  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -17.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -17.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -17.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -14.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -17.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -17.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -17.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -18.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -19.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -15.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -20.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -21.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -21.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -21.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -21.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -17.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -18.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -18.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -22.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -23.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -23.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -23.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -23.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -23.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -23.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -23.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -23.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -23.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -23.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -23.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -23.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -24.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -24.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -24.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -24.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -24.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -24.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -19.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -25.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -25.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -25.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -26.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -26.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -20.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -21.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -22.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -22.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -23.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -23.2]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -23.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -23.2]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -23.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -23.2]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -23.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -23.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -23.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -23.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -24.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -24.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -24.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -24.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -24.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -24.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -26.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -25.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -27.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -27.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.7  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.7  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.7  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.7  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.7  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.8]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -27.7  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.8]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -26.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -26.8]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -27.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -28.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -28.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -27.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -28.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -28.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.7  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.7  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.7  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.7  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.7  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.7  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.7  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -28.7  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.8  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -28.8  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.7]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.8]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -28.9]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29. ]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.1]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.2]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.3]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -28.9  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.4]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.5]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -29.   89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -29.1  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -29.2  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.3  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -29.3  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -29.4  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -29.5  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "After update:  [[  0.    1.    0.    0.  -29.6  89.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "After update:  [[  1.    0.    0.    0.   61.2 -29.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Number of fails:\n",
      "92\n",
      "\n",
      "Number of rewards:\n",
      "408\n",
      "Real state update:  1  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1  0.   0. ]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -1.   0. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.8  0.1  0.1  0.   0. ]]\n",
      "After update:  [[ 0.1  0.8  0.1  0.1 -3.   0. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.8  0.   0. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.8  0.  -3. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.8  0.1  0.   0. ]]\n",
      "After update:  [[ 0.1  0.1  0.8  0.1 -1.   0. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.1  0.8  0.1  0.1 -3.   0. ]]\n",
      "After update:  [[ 0.1  0.8  0.1  0.1 -3.   9. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -1.   0. ]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -1.  -3. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 9.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -1.  -3. ]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -1.2 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.1  0.8  0.1  0.1 -3.   9. ]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -3.   17.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 17.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -1.2 -3. ]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -0.7 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -3.   17.1]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -3.   24.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -0.7 -3. ]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -1.6 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 24.4]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.1  0.1  0.8  0.1 -1.   0. ]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  -1.   11. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 24.4]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -1.6 -3. ]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -0.5 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -3.   24.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -3.   31. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-0.5]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -0.5 -3. ]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -1.5 -3. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 11.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -1.5 -3. ]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -3.5 -3. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 11.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -3.5 -3. ]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -3.5 -4.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 31.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -3.5 -4.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -1.7 -4.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 11.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -3.   31. ]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -4.8  31. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 31.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -1.7 -4.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -0.  -4.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -4.8  31. ]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -4.8  36.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 11.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -0.  -4.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -0.1 -4.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  -1.   11. ]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  -1.   18.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 18.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -0.1 -4.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -0.1 -5.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -0.1 -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -1.1 -5.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 36.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.8  0.  -3. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.8 -0.  -3. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 36.9]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -1.1 -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1  1.  -5.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -4.8  36.9]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -4.8  42.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 42.2]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1  1.  -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1  3.3 -5.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-0.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -4.8  42.2]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -4.8  47. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1  3.3 -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1  2.  -5.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 2.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.1  0.1  0.1  0.8 -0.  -3. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.8 -2.8 -3. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 2.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1  2.  -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1  1.  -5.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 18.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1  1.  -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1 -0.6 -5.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 47.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1 -0.6 -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1  2.2 -5.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -4.8  47. ]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -4.8  51.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 18.9]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1  2.2 -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1  2.5 -5.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  -1.   18.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  -1.   25.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 51.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1  2.5 -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1  5.3 -5.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -4.8  51.1]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -4.8  54.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -4.8  54.8]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -4.8  48.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 48.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -4.8  48.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  48.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  48.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  52.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 52.3]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1  5.3 -5.8]]\n",
      "After update:  [[ 0.8  0.1  0.1  0.1  8.  -5.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 52.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  52.3]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  60.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 60.3]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.8  0.1  0.1  0.1  8.  -5.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  11.   -5.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  60.3]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  63. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 63.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  11.   -5.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  13.9  -5.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  63. ]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  65.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-2.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  13.9  -5.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  13.9  -8.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  13.9  -8.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  13.9  -8.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  13.9  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  16.8  -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  65.5]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  67.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  67.7]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  60. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 60.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.1  0.8 -2.8 -3. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.8  1.3 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 16.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  60. ]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  64.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 64.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  16.8  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  19.3  -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 1.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  64.3]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  67. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 19.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  67. ]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  58.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 58.8]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.1  0.8  1.3 -3. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.8  4.9 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  58.8]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  62.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 19.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  19.3  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  17.9  -8.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 25.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  17.9  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  15.2  -8.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 62.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  15.2  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  17.7  -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  62.3]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  65.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  17.7  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  20.2  -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  65.5]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  68.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 68.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  20.2  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  22.6  -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  68.3]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  70.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  22.6  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  25.   -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  70.9]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  73.2]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 73.2]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.1  0.8  4.9 -3. ]]\n",
      "After update:  [[ 0.1  0.1  0.1  0.8  9.3 -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 25.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  73.2]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  76.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  25.   -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  27.7  -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 9.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  76.9]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  79. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 79.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  27.7  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  30.3  -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 9.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  79. ]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  80.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 80.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  30.3  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  32.7  -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 9.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  80.8]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.4  82.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 82.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  32.7  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  35.   -8.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 25.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.4  82.5]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -2.2  82.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 82.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  35.   -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  37.1  -8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 9.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -2.2  82.5]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -2.2  84. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  -1.   25.8]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  -1.   22.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.1  0.1  0.1  0.8  9.3 -3. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  14.1  -3. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 14.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -2.2  84. ]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -2.2  85.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 85.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  37.1  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  39.2  -8.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 14.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -2.2  85.7]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -3.9  85.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 39.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  39.2  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  37.4  -8.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 22.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  37.4  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  32.4  -8.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 22.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  -1.   22.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  -1.   18.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 32.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  32.4  -8.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  32.4  -8.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 18.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  32.4  -8.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  32.4  -8.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 14.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  32.4  -8.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  29.3  -8.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 14.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  14.1  -3. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  10.8  -3. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 85.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  29.3  -8.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  32.2  -8.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 32.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -3.9  85.7]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -3.9  88.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 88.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  32.2  -8.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  35.1  -8.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 10.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -3.9  88.7]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -3.9  89.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  35.1  -8.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  37.8  -8.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 18.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -3.9  89.7]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -3.9  91.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 10.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  37.8  -8.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  33.9  -8.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 10.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  10.8  -3. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  10.8   7.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 91.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  33.9  -8.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  36.8  -8.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -3.9  91.2]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -3.9  98.4]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 36.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  -1.   18.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  -1.   17. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  36.8  -8.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  36.8  -9.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  36.8  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  40.   -9.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 10.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -3.9  98.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -5.6  98.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  40.   -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  42.9  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 10.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -5.6  98.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -5.6  98.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  42.9  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  45.5  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 10.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -5.6  98.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -5.6  98.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  10.8   7.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  16.6   7.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 16.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -5.6  98.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -5.6  98.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  45.5  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  47.9  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 16.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -5.6  98.9]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -5.6  99.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 16.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  47.9  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  43.4  -9.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 16.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  16.6   7.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  13.3   7.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 13.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  43.4  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  39.1  -9.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 99.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  13.3   7.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  16.9   7.2]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 99.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  16.9   7.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  22.2   7.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -5.6  99.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -5.6  101.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 101.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  22.2   7.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  27.1   7.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 27.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -5.6  101.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -5.6  102.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 27.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  39.1  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  36.4  -9.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 36.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  27.1   7.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  24.3   7.2]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 102.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  24.3   7.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  29.1   7.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 17.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -5.6  102.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -5.6  102.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 102.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  36.4  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  40.   -9.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 17.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -5.6  102.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -6.7  102.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  29.1   7.2]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  29.1   4.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 102.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  40.   -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  43.2  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -6.7  102.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -6.7  103.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  43.2  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  46.2  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -6.7  103.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -6.7  104.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  29.1   4.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  33.6   4.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 33.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -6.7  104.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -6.7  105.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  46.2  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  49.1  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 33.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -6.7  105.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -6.7  107. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  33.6   4.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  37.8   4.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 107.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -6.7  107. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -0.5  107. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  49.1  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  51.8  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -0.5  107. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -0.5  108.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -0.5  108.3]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -0.5  95.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 95.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  51.8  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  53.3  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -0.5  95.8]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -0.5  98.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  53.3  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  54.8  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -0.5  98.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -0.5  100.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 37.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  54.8  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  51.3  -9.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 37.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  37.8   4.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  34.    4.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 100.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  34.    4.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  37.6   4.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 100.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -0.5  100.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -0.5  107.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  51.3  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  53.8  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -0.5  107.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -0.5  108.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 108.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  53.8  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  56.1  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -0.5  108.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -0.5  109.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  56.1  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.3  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -0.5  109.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -0.5  110.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  37.6   4.8]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  37.6   2.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 110.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.3  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.3  -9.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 17.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -0.5  110.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  110.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 110.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.3  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.1  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  110.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  111.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 111.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  37.6   2.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  37.6   8.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.1  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.8  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  111.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  112.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 17.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.8  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.8  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  -1.   17. ]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  -1.   27.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.8  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.   -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 27.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  112.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  112.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.   -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.   -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  112.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  113.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.   -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.9  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  113.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  113.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.9  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  65.6  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  113.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  114.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 27.3]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  114.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  102.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 102.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  65.6  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.2  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 66.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  102.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  106.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 37.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.2  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.6  -9.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 27.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  37.6   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  33.    8.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 106.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.6  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.9  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 33.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  106.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  107.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.9  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.2  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 64.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  107.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  110.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 27.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.2  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.   -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 33.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  -1.   27.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  -1.   36.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 110.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.   -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.   -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 33.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  110.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  111.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 36.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  -1.   36.2]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  -1.   32.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.   -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.8  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 33.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  111.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  111.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 33.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.8  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.2  -9.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 32.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  33.    8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  29.3   8.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.2  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.3  -9.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 29.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  111.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  112. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 32.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  112. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  100.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 60.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.3  -9.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.3  -6.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 100.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  -1.   32.5]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1   6.1  32.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 100.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  100.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  107.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 32.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  29.3   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  28.    8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 28.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1   6.1  32.5]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1   6.1  40.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 40.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.3  -6.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  56.5  -6.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 40.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1   6.1  40.5]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1   5.7  40.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  56.5  -6.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.4  -6.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 28.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  107.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  107.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 40.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  107.9]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -2.1  97.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 28.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -2.1  97.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -2.1  86.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 40.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.4  -6.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  54.8  -6.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 28.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1   5.7  40.5]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1   5.7  47.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 86.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  28.    8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  31.2   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 31.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -2.1  86.9]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -2.1  89.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  54.8  -6.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  55.5  -6.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 31.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -2.1  89.7]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -2.1  92.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 92.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  55.5  -6.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  56.3  -6.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 31.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -2.1  92.2]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -2.1  94.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 94.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  56.3  -6.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.2  -6.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 94.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -2.1  94.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -2.1  101.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 101.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  31.2   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  35.2   8.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 47.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -2.1  101.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -1.1  101.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 101.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.2  -6.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.6  -6.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 47.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -1.1  101.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -1.1  104.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 47.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.6  -6.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.6  -5.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 58.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -1.1  104.3]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.1  95.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 47.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1   5.7  47.7]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1   5.7  43.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 43.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.6  -5.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  55.2  -5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 35.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1   5.7  43.7]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1   5.7  51.1]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 95.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  35.2   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  38.3   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 38.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.1  95.6]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.1  98.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 55.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  55.2  -5.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  53.1  -5.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 51.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  53.1  -5.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  48.9  -5.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  48.9  -5.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  50.9  -5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 51.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.1  98.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -1.1  101.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 101.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  38.3   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  41.6   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -1.1  101.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -1.1  103.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 51.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  50.9  -5.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  48.9  -5.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1   5.7  51.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1   5.7  58.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 58.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  48.9  -5.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  48.9  -3.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 58.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  48.9  -3.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  48.9  -1.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 58.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1   5.7  58.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1   5.7  54.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 54.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  48.9  -1.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  48.9   0.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 54.1]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -1.1  103.6]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.1  94.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 94.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  48.9   0.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  50.6   0.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.1  94.6]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  -1.1  97.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 54.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  50.6   0.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  50.6   1.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 97.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  50.6   1.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  52.3   1.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  -1.1  97.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -1.1  100.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 100.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  52.3   1.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  54.1   1.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 54.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -1.1  100.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   -1.1  103.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  54.1   1.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  56.    1.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 54.1]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   -1.1  103.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  103.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  56.    1.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.7   1.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  103.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  105.4]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.8   0.1   5.7  54.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  12.6  54.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  105.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  107.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 54.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.7   1.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.7   2.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.7   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.5   2.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  107.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  108.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 108.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.5   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.3   2.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  108.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  110.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 54.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.3   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.5   2.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 58.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  12.6  54.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  12.6  62.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 110.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.5   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.5   2.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  110.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  111.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.5   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.4   2.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  111.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  112.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 62.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  112.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  103.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 41.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.4   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.5   2.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 62.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  41.6   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  39.4   8.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.5   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.9   2.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  103.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  105.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.9   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.3   2.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  105.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  106.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 106.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.3   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.7   2.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  106.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  108.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 62.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  108.3]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1   0.3  99.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 62.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  39.4   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  39.5   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 99.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  12.6  62.4]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  12.6  73.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 99.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.7   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.4   2.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1   0.3  99.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  103.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 63.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.4   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.1   2.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 73.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.1   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.8   2.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.8   2.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.8   5.4]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  12.6  73.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  12.6  68.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.8   5.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.3   5.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  103.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  105.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.3   5.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.8   5.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  105.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  107. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.8   5.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.3   5.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  107. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  108.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 108.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.3   5.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.8   5.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  108.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  109.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.8   5.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  65.2   5.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  109.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    0.3  111. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  65.2   5.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  65.2   5. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  65.2   5. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.6   5. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 68.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    0.3  111. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    2.8  111. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.6   5. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.8   5. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    2.8  111. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    2.8  112.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.8   5. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.    5. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    2.8  112.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    2.8  113. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 68.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    2.8  113. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    2.8  104.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 68.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.    5. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.6   5. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  12.6  68.6]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  12.6  73.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.6   5. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.3   5. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 104.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    2.8  104.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    7.9  104.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.3   5. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.9   5. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    7.9  104.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    7.9  108.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 108.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.9   5. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.8   5. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    7.9  108.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    7.9  110. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.8   5. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.8   7.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 110.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.8   7.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.7   7.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    7.9  110. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    7.9  111.2]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    7.9  111.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    7.9  103. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.7   7.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.6   7.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  12.6  73.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  14.3  73.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.6   7.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.1   7.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    7.9  103. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    7.9  104.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 39.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  14.3  73.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  14.3  66.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  39.5   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  42.9   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 104.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    7.9  104.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    7.9  111.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 42.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.1   7.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.1   7.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.1   7.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.2   7.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 42.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    7.9  111.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    7.9  113.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  14.3  66.7]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.9  66.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 66.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    7.9  113.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  113.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 66.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.9  66.7]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.9  62.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 62.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.2   7.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.3   7.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 42.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.9  62.4]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.9  68.6]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 42.9]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  113.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  102.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 102.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.3   7.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.8   7.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 42.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  102.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  104.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 68.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  104.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1   9.4  96.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 96.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.8   7.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.8  11.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 68.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.8  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.6  11.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 42.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.9  68.6]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.9  74.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 96.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.6  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.9  11.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 42.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1   9.4  96.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1   9.4  99.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 99.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.9  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  65.3  11.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 42.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1   9.4  99.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  101.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 42.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  65.3  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.2  11.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 74.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  42.9   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  41.5   8.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 101.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.2  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.2  11.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  101.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  103.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.2  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.3  11.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  103.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  105.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 74.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.3  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.9  11.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.9  74.2]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.9  79.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 41.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.9  79.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.9  71.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 71.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.9  71.5]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.9  67.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 61.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.9  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.7  11.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 67.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.7  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  56.1  11.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  56.1  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.   11.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  105.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  107.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.   11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.8  11.1]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 59.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  107.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   10.2  107.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.8  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.4  11.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   10.2  107.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   10.2  109.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 109.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.4  11.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.4  15.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.4  15.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.   15.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 41.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   10.2  109.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   10.2  110.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 67.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.   15.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.   16.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 63.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   10.2  110.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   10.2  101.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 63.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.   16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.7  16.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 41.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.7  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  55.   16.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 55.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  41.5   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  40.8   8.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 67.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  55.   16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  51.9  16.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 67.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.9  67.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.9  62.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 62.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   10.2  101.5]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  10.2  93.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 93.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  51.9  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  53.2  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 62.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  10.2  93.4]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1  10.2  98.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  53.2  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  54.7  16.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.8   0.1   0.1  10.2  98.1]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1   9.4  98.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 98.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  54.7  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  56.1  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 62.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1   9.4  98.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  102.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 102.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  56.1  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.7  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  102.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  104.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.7  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.3  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  104.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  106.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 62.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.3  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.4  16.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 62.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.9  62.8]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.8  62.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 62.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  106.1]]\n",
      "After update:  [[  0.1   0.8   0.1   0.1   9.4  97.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 97.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.4  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.5  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 62.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.8   0.1   0.1   9.4  97.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  101.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 101.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.5  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.8  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  101.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  103.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 62.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.8  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.8  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.8  62.8]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.8  68.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.8  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.3  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  103.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  105.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 68.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.3  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.9  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.8  68.8]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.8  74.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.9  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.6  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  105.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  107.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.6  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.2  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  107.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  109. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 40.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.2  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.3  16.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 74.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  40.8   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  39.7   8.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.3  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.3  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  109. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  110.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 110.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.3  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.2  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  110.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1    9.4  111.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.2  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.   16.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 74.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1    9.4  111.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  111.4]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 74.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.8  74.2]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.8  69.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.   16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.6  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  111.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  112.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  39.7   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  43.7   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 43.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  112.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  113.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.6  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.2  16.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 43.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  113.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  114.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 69.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.2  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.2  17.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 43.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  114.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  103.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 103.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.2  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.9  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 43.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  103.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  105.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 105.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.9  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.7  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 43.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  105.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  107.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 69.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.8  69.7]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  20.8  65.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.7  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.6  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  107.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  111.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.6  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.6  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 43.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  111.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  112.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.6  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.6  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 43.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  112.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  113.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  43.7   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  47.4   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 47.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  113.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  115.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 115.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  20.8  65.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  26.9  65.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 47.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  115.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  116.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 47.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.6  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.6  16.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 47.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.6  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.3  16.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 65.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  47.4   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  44.9   8.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.3  16.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.3  17.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 116.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.3  17.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.   17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 44.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  116.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  117.4]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 117.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  26.9  65.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  26.9  65.2]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  117.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  107.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.   17.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.8  17.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 44.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  107.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  109.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  26.9  65.2]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  26.9  60.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 60.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.8  17.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.8  17.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 109.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.8  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.7  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 44.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  109.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  111.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.7  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.6  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 44.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  111.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  112.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 60.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.6  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.4  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 112.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  26.9  60.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  26.9  72.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 72.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.4  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  65.5  17.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 112.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  26.9  72.8]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  72.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  65.5  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.   17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 44.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  112.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  114.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 114.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.   17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.4  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 44.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  114.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  115.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 115.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  44.9   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  48.6   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 115.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  115.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  122. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.4  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.3  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 48.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  122. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  122.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.3  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.1  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 48.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  122.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  123.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 72.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  123.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  113.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 72.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.1  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.7  17.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 72.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.7  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  65.6  17.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 113.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  65.6  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.1  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 72.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  113.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  117.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 67.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.1  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.8  17.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 117.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.8  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.7  17.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 64.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.7  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.4  17.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 72.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.4  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.   17.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 72.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  72.8]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  68.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 117.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.   17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.5  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 48.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  117.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  118.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 118.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.5  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.8  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  118.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  120.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 120.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.8  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.1  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 66.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  120.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  122.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.1  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.3  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 48.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  122.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  123.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 68.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  68.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  63.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.3  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.3  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 123.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  123.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  130. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  48.6   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  53.1   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  130. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  130.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 70.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.3  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.9  17.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 67.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.9  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.5  17.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  63.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  58.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.5  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.6  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  130.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   11.4  130.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 58.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.6  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.6  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  58.8]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  66.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.6  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.7  17.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 130.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   11.4  130.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  130.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.7  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.5  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  130.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  130.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 69.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  66.2]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  62.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.5  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.   17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  130.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  130.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 62.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  62.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  57.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.   17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.3  17.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  130.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  131. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 57.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.3  17.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.3  17.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 57.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.3  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.5  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  57.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  65.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 131.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.5  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.9  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  131. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  132.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.9  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.9  17.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 132.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.9  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.2  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 132.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  132.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  138.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 75.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.2  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.7  17.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.7  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.7  17.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 138.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.7  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  71.   17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  138.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  137.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  137.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  126.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 126.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  71.   17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  73.   17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  126.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  126.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 126.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  73.   17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.9  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  126.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  127.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 127.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.9  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.6  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  127.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  128. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 128.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.6  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  78.2  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  128. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  128.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 128.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  53.1   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  57.1   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  128.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  129.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 129.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  78.2  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  79.8  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 57.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  129.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  130.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 79.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  79.8  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.2  17.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 130.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.2  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.9  17.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.9  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  78.6  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 57.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  130.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  130.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 57.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  78.6  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.3  17.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 57.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  57.1   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  53.    8.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  53.    8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  57.2   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 57.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  130.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  131.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 131.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.3  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.4  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 57.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  131.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  131.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.4  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  73.9  17.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  73.9  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.7  17.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  65.4]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  61.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 131.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.7  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  71.4  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 57.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  131.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  132.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 61.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  71.4  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.1  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 68.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  61.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  69.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 132.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.1  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.9  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  132.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  133.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 69.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  69.4]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  65. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  133.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  122.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.9  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.6  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 57.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  122.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  123.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  57.2   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  60.4   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 60.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  123.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  125.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  125.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  115. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 115.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.6  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  73.5  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 60.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  115. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  117.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 117.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  73.5  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.5  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 60.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  117.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  119.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  119.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  109.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  109.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   17.7  100.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 100.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.5  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.1  17.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 65.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   17.7  100.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  100.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 60.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  65. ]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  60.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 100.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.1  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  73.8  17.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 60.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  100.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  104.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 104.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  60.4   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  61.7   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 61.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  104.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  108.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 60.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  73.8  17.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  73.8  17.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 108.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  73.8  17.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.1  17.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 74.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  108.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  112.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 112.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.1  17.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.7  17.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 112.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  112.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  119. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  61.7   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  64.    8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 64.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  119. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  121.2]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 60.3]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  121.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  110.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 110.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  64.    8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  65.5   8.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  110.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  114.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 114.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.7  17.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.4  17.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  114.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  116.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 60.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.4  17.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.4  17.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 60.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  60.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  56.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 116.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.4  17.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.2  17.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  116.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  119.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.2  17.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.1  17.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 77.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  119.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  122.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 56.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  56.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  52. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  65.5   8.4]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  65.5   8.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 77.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  65.5   8.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  64.1   8.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.1  17.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.5  17.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  122.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   18.1  111.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.5  17.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.5  17.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 70.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.5  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.1  17.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.1  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.4  17.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.4  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.1  17.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 111.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   18.1  111.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   22.2  111.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 111.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.1  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  65.6  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 111.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   22.2  111.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   22.2  118.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   22.2  118.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   22.2  107.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 65.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  65.6  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.3  17.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.3  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.1  17.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 58.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.1  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  55.9  17.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  55.9  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  51.5  17.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 51.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  51.5  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  49.5  17.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  49.5  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  45.7  17.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 107.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  45.7  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  48.7  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 107.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   22.2  107.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   22.2  114.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 114.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  64.1   8.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  65.8   8.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 48.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   22.2  114.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   20.9  114.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 114.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  48.7  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  52.   17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   20.9  114.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   20.9  117.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 117.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  52.   17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  55.2  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   20.9  117.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   20.9  119.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  55.2  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.3  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 52.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   20.9  119.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   20.9  121. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  65.8   8.7]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  65.8   9. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 52.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.3  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  55.6  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  52. ]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  61.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 121.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  55.6  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  58.7  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   20.9  121. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   20.9  123.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  58.7  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.7  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   20.9  123.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   20.9  125.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 125.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.7  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.5  17.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   20.9  125.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   20.9  126.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 61.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.5  17.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.5  17.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 126.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.5  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.2  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   20.9  126.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   20.9  128.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 67.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.2  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.9  17.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.9  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.7  17.4]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 61.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  61.1]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  56.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 128.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.7  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.9  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   20.9  128.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   20.9  129.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 56.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.9  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.1  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  56.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  65.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 129.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.1  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.4  17.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   20.9  129.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   21.1  129.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   21.1  129.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   21.1  119.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  65.5]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  61.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 65.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.4  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.2  17.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 61.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  65.8   9. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  61.1   9. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 119.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  61.2]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  61.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.2  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.5  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 64.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   21.1  119.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   21.1  121.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 61.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.5  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.5  17.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 61.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.5  17.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.9  17.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 61.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  61.1   9. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  56.9   9. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 56.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.9  17.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.3  17.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 61.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  56.9   9. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  53.1   9. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 61.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  61.6]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  57.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 121.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.3  17.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.1  17.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   21.1  121.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   21.1  122.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.1  17.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  59.1  17.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 57.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  53.1   9. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  49.4   9. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 49.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  59.1  17.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  56.1  17.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 57.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  49.4   9. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  46.1   9. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 57.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  56.1  17.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  56.1  17.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 57.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  56.1  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  54.1  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 54.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  57.4]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  65. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  46.1   9. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  50.3   9. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 65.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   21.1  122.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   21.2  122.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.5]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   21.2  122.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.9  122.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 50.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.9  122.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.9  123.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  50.3   9. ]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  50.3  10.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  54.1  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  57.6  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 50.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.9  123.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.9  124. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  57.6  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.8  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 50.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.9  124. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.9  124.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.8  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.7  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 50.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.9  124.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.9  125.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 65.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  65. ]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  60.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 125.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.7  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.3  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 50.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.9  125.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.9  125.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 125.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.3  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.7  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 50.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.9  125.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.9  126.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 68.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.7  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.3  17.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 60.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.3  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  61.5  17.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 60.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  50.3  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  49.1  10.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 49.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  60.7]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  67.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 126.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  61.5  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.4  17.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 64.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.9  126.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.9  127.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 67.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.4  17.4]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.4  18.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 127.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.9  127.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.9  122.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 67.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.4  18.1]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.4  18.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.4  18.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.7  18.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 67.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.9  122.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  122.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.7  18.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.8  18.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 49.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  122.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  122.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.8  18.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.7  18.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 49.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  122.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  123.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.7  18.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.5  18.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 49.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  123.4]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  124. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.5  18.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.2  18.7]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 67.6]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  124. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  124. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  49.1  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  53.1  10.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  124. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  124.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 74.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.2  18.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.2  19.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 74.2]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  124.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  115.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 115.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.2  19.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.   19.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 115.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  115.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  122. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.   19.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.3  19.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 67.6]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  122. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  124.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 67.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.3  19.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.3  20.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 67.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  67.6]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  30.2  63.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.3  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.6  20.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  124.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  125. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 125.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  125. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  119.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 119.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.6  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  78.4  20.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 78.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  119.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  122.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 63.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  78.4  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.6  20.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 63.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  30.2  63.2]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  29.2  63.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 63.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.6  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.6  20.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.6  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.   20.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  122.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  123.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 63.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  29.2  63.2]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  29.2  58.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.   20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.3  20.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 123.8]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  123.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  130.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 77.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.3  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.8  20.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 74.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.8  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.3  20.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 70.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.3  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.9  20.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 58.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.9  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  62.8  20.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 58.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  62.8  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  60.2  20.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  29.2  58.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  29.2  66.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  60.2  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.6  20.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  130.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.2  130.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 66.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.6  20.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.6  20.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 66.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.6  20.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  63.6  20.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  63.6  20.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.7  20.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 66.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.2  130.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   26.8  130.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 66.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.7  20.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.3  20.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  29.2  66.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  29.2  72.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.3  20.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.3  20.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   26.8  130.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   26.8  130.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.3  20.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  70.   20.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   26.8  130.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   26.8  132.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 132.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  70.   20.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.6  20.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   26.8  132.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   26.8  132.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.6  20.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.6  20. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 132.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.6  20. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.9  20. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 72.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   26.8  132.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   26.8  133.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.9  20. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.1  20. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   26.8  133.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   26.8  133.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 133.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  29.2  72.9]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  29.2  73.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.1  20. ]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.1  20.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.1  20.9]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  79.1  20.9]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 73.3]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   26.8  133.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.   133.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 73.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  79.1  20.9]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  79.1  21.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  29.2  73.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   73.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.   133.7]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.   133.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  53.1  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  57.5  10.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 57.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.   133.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.   133.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 73.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  79.1  21.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.1  21.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 57.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   73.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   79.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  57.5  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  61.5  10.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 61.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.   133.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.   134.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 134.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.1  21.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  78.2  21.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 61.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.   134.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.   134.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 79.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  78.2  21.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.7  21.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 134.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   79.6]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   91.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 134.8]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.   134.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.   129.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 91.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.7  21.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.7  23.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 91.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.7  23.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.4  23.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 61.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   91.4]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   96.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 129.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.4  23.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.3  23.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 96.2]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.   129.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.   132.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 61.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.3  23.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.6  23.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 96.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  61.5  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  60.   10.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 96.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   96.2]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   91.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 132.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.6  23.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.   23.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 91.3]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.   132.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   28.6  132.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 132.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.   23.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.1  23.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 60.]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   28.6  132.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  132.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 91.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.1  23.8]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.1  25.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 91.3]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  132.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  123.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 123.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.1  25.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  78.3  25.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 60.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  123.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  125.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 125.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  60.   10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  63.   10.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  125.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  129.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 129.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  78.3  25.7]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  78.3  30.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 91.3]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  129.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  120.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 120.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  78.3  30.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  79.1  30.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  120.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  122.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 122.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  63.   10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  65.5  10.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  122.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  124.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 91.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  79.1  30.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.5  30.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   91.3]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   96.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 96.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.5  30.5]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.5  32.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 77.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.5  32.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.5  32.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 77.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.5  32.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.   32.2]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 124.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.   32.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.5  32.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 74.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.5  32.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.   32.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.   32.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.   43.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 96.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.   43.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.   43.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 65.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.   43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.   43.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 96.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  65.5  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  63.7  10.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 69.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.   43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  66.6  43.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 96.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  66.6  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  64.7  43.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 124.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  64.7  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  67.2  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  124.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  126.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 126.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  67.2  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.6  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  126.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  127.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 127.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.6  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  71.8  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  127.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  128.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 128.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  71.8  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  73.9  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  128.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  130.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 130.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  73.9  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.9  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 75.9]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  130.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  132.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 132.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.9  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.9  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  132.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  133.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 96.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   96.4]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   91.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 91.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.9  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.4  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 91.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   91.5]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   98.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.4  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  78.4  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  133.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  133.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 98.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   98.7]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   93.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 133.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  78.4  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  80.3  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  133.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  134.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 80.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  80.3  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  77.7  43.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 93.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  77.7  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  74.4  43.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 134.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  74.4  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  76.7  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 76.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  134.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  136.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 136.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  76.7  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  78.9  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  136.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  136.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 136.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  78.9  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  81.   43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  136.8]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   27.5  137.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 137.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  81.   43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  82.9  43.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 93.7]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   27.5  137.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   29.2  137.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 93.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   93.7]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   88.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 137.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  82.9  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  84.6  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   29.2  137.2]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   29.2  137.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  84.6  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  81.9  43.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  81.9  43.6]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  81.9  53.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 88.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   88.8]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   84. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 137.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  81.9  53.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  83.7  53.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 63.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   29.2  137.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   29.2  137.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 137.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  63.7  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  67.4  10.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 67.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   29.2  137.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   29.2  138.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 138.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  83.7  53.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  85.4  53.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 67.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   29.2  138.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   29.2  139. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 139.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  85.4  53.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  87.   53.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 67.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   29.2  139. ]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   29.2  139.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 139.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  87.   53.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  88.5  53.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 67.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   29.2  139.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   29.2  139.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 139.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  88.5  53.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  88.5  56.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 84.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  88.5  56.2]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  88.5  54.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 67.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  88.5  54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  84.   54.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 84.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  67.4  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  64.4  10.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  84.   54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  81.3  54.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 64.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  81.3  54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  75.3  54.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 64.4]\n",
      "Reward:  -30\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   29.2  139.9]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   29.2  128.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  75.3  54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  73.5  54.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 64.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   84. ]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   89.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 128.1]\n",
      "Reward:  -10\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   29.2  128.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   35.5  128.1]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 128.1]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   35.5  128.1]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   35.5  134.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 89.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  73.5  54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.3  54.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 64.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   89.8]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   95. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 64.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.3  54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.2  54.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 134.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  64.4  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  65.7  10.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 134.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.2  54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.   54.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.7]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   35.5  134.5]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   35.5  135.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 65.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  72.   54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  69.1  54.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 69.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  65.7  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  61.7  10.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 135.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.1   0.1   0.1   0.8  61.7  10.3]]\n",
      "After update:  [[  0.1   0.1   0.1   0.8  65.4  10.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 135.3]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   35.5  135.3]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   35.5  141.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 95.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  69.1  54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.8  54.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   95. ]]\n",
      "After update:  [[  0.1   0.1   0.8   0.1  36.   99.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 99.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.8  54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  68.9  54.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.1   0.1   0.8   0.1  36.   99.7]]\n",
      "After update:  [[   0.1    0.1    0.8    0.1   36.   104. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 141.6]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.8   0.1   0.1   0.1  68.9  54.3]]\n",
      "After update:  [[  0.8   0.1   0.1   0.1  72.3  54.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 65.4]\n",
      "Reward:  90\n",
      "Before update:  [[   0.1    0.8    0.1    0.1   35.5  141.6]]\n",
      "After update:  [[   0.1    0.8    0.1    0.1   35.5  141.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Number of fails:\n",
      "223\n",
      "\n",
      "Number of rewards:\n",
      "277\n",
      "Real state update:  1  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.5  0.2  0.2  0.   0. ]]\n",
      "After update:  [[ 0.2  0.5  0.2  0.2 -1.   0. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2  0.   0. ]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -3.   0. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2  0.   0. ]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -1.   0. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -3.   0. ]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -3.   9. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 9.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -1.   0. ]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -1.  -2.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.5  0.2  0.2 -1.   0. ]]\n",
      "After update:  [[ 0.2  0.5  0.2  0.2 -1.  -3. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-1.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -1.  -2.3]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -2.  -2.3]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 9.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.5  0.2  0.2 -1.  -3. ]]\n",
      "After update:  [[ 0.2  0.5  0.2  0.2 -3.2 -3. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-2.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -3.   9. ]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -3.   4.9]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-2.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.5  0.2  0.2 -3.2 -3. ]]\n",
      "After update:  [[ 0.2  0.5  0.2  0.2 -4.  -3. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -2.  -2.3]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -4.4 -2.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-3.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5  0.   0. ]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -1.2  0. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.2  0.5  0.2  0.2 -4.  -3. ]]\n",
      "After update:  [[ 0.2  0.5  0.2  0.2 -4.   6.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 6.3]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -3.   4.9]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -3.2  4.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 0.]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.2  0.5  0.2  0.2 -4.   6.3]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   14.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -1.2  0. ]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -1.2 -2.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -4.4 -2.3]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -4.4 -4.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 14.7]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -3.2  4.9]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -2.7  4.9]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-1.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   14.7]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   22.1]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-4.4]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -1.2 -2.6]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -2.4 -2.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -4.4 -4.7]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -6.6 -4.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 22.1]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -2.4 -2.6]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -1.4 -2.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-1.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   22.1]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   28.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   28.8]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   23.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   23.3]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   18.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-1.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   18.4]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   13.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-4.7]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -6.6 -4.7]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -6.6 -7.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 4.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -2.7  4.9]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -2.7  1.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-6.6]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -6.6 -7.6]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -7.5 -7.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 13.4]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -7.5 -7.6]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -8.7 -7.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 1.8]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -8.7 -7.6]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -8.7 -9.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 1.8]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -8.7 -9.7]]\n",
      "After update:  [[ 0.5  0.2  0.2  0.2 -8.7 -9.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 13.4]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -2.7  1.8]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  -2.7  11.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-1.4]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.5  0.2  0.2  0.2 -8.7 -9.7]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -8.7 -11.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-1.4]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -1.4 -2.6]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -2.4 -2.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 11.7]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -2.4 -2.6]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -4.2 -2.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 13.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  -2.7  11.7]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  -2.4  11.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   13.4]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   20.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 20.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -8.7 -11.8]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -7.2 -11.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-2.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   20.9]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   27.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 11.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  -2.4  11.7]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -2.4  8.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-7.2]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -4.2 -2.6]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -4.2 -5.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 27.6]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -2.4  8.5]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -1.   8.5]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 8.5]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   27.6]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   34.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 8.5]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -4.2 -5.9]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -4.2 -7.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-7.2]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -4.2 -7.6]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -5.4 -7.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 8.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -7.2 -11.8]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -8.8 -11.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-5.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -8.8 -11.8]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -9.4 -11.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [-5.4]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -5.4 -7.6]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -8.3 -7.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-9.4]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -1.   8.5]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -1.   3.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 3.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -8.3 -7.6]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -8.3 -9.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 3.9]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -1.   3.9]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -1.   0.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 0.8]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -1.   0.8]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -1.  -2.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-9.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -9.4 -11.8]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2 -10.2 -11.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 34.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2 -10.2 -11.8]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -9.4 -11.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 34.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -9.4 -11.8]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -6.7 -11.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-8.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   34.5]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   39.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-8.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -6.7 -11.8]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -7.7 -11.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-8.3]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -8.3 -9.5]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -8.3 -0.2]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-1.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -8.3 -0.2]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -8.3 -3.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-3.3]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -8.3 -3.3]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -8.7 -3.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 39.4]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -8.7 -3.3]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -8.7  9.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 39.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -7.7 -11.8]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -7.7 -10.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-1.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -7.7 -10.5]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -8.  -10.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [-1.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -1.  -2.2]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -4.  -2.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-8.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -8.  -10.5]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -8.  -13.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-2.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -8.  -13.1]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -8.  -15. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-8.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -8.7  9.2]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -8.7  4.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-8.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -4.  -2.2]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -4.  -5.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 39.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -8.  -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -5.  -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 4.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   39.4]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -4.   44.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 44.8]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -4.  -5.6]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -1.  -5.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 44.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -4.   44.8]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   44.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-5.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -5.  -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -5.9 -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [-1.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -5.9 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -8.4 -15. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-1.]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -8.7  4.6]]\n",
      "After update:  [[ 0.2  0.2  0.2  0.5 -8.9  4.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [-1.]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -1.  -5.6]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -4.  -5.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 4.6]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -4.  -5.6]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -4.2 -5.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 44.8]\n",
      "Reward:  90\n",
      "Before update:  [[ 0.2  0.2  0.2  0.5 -8.9  4.6]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -8.9  16.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 44.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -8.4 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -5.  -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 44.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   44.8]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   52.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 52.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -5.  -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -1.3 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 16.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   52.9]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   57.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [-4.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -1.3 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  -2.5 -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 16.7]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -4.2 -5.6]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -5.4 -5.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 57.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -8.9  16.7]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -8.9  16.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 57.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  -2.5 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   1.4 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 1.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   57.9]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   61.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 61.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   1.4 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   5.2 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   61.2]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   64.5]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 64.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -8.9  16.7]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.8  16.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   64.5]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   67.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 67.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   5.2 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   9.1 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-5.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   67.5]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   69.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 69.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   9.1 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  12.7 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 12.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   69.3]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   72.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 16.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  12.7 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  11.8 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [-5.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.8  16.7]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.8  23.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 11.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  11.8 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  10.6 -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [-5.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  10.6 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   6.1 -15. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 6.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   6.1 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   5.  -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 5.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   5.  -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   1.9 -15. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 72.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   1.9 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   6.5 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 23.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   72.4]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   76. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 76.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.8  23.6]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.8  24.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 76.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   76. ]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   71.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 24.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   6.5 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   6.8 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 24.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.8  24.3]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.8  32.8]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 71.5]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -5.4 -5.6]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2 -0.1 -5.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 32.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   71.5]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   76. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 76.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   6.8 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  11.2 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 32.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   76. ]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   80. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 80.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  11.2 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  15.5 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 32.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   80. ]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   83.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [-0.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.8  32.8]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.8  26.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 83.6]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2 -0.1 -5.6]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2  5.6 -5.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 5.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   83.6]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   84.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 5.6]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2  5.6 -5.6]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2  4.5 -5.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 84.7]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2  4.5 -5.6]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2  7.8 -5.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 7.8]\n",
      "Reward:  -30\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2  7.8 -5.6]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2  7.8 -7.4]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 26.5]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2  7.8 -7.4]]\n",
      "After update:  [[ 0.2  0.2  0.5  0.2  8.1 -7.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 84.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.8  26.5]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.8  39.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 8.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.8  39.6]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.8  33.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 84.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  15.5 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  19.7 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 33.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   84.7]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   87.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 87.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  19.7 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  23.8 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 33.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   87.9]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   90.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 90.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  23.8 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  27.7 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 27.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   90.8]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   92.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 8.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.8  33.3]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.8  27.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 27.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  27.7 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  26.1 -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 8.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  26.1 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  21.1 -15. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 8.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   92.9]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   81.3]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 81.3]\n",
      "Reward:  -10\n",
      "Before update:  [[ 0.2  0.2  0.5  0.2  8.1 -7.4]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  12.8  -7.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 12.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   81.3]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   83.2]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 83.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  12.8  -7.4]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  17.2  -7.4]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 27.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   83.2]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   86.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   86.1]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   75.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.8  27.6]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.8  23.2]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   75.9]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   66.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 66.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  21.1 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  23.3 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 23.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   66.7]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   70.9]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.8  23.2]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.   23.2]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  17.2  -7.4]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  17.2   8. ]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 70.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   70.9]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   66.5]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 66.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  23.3 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  25.3 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 23.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   66.5]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   70.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.   23.2]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -3.   19.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 19.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -3.   19.3]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  19.3]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 19.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  19.3]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  27.9]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  17.2   8. ]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  15.9   8. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 70.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  15.9   8. ]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  17.    8. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  27.9]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  23.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   70.7]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   62.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   62.7]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   55.5]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 55.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  17.    8. ]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  18.7   8. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 25.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   55.5]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   61. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 18.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  23.5]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  19.6]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 18.7]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   61. ]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   53.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 53.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  25.3 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  26.  -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 53.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   53.4]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   61.3]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 26.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  19.6]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  16.7]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 16.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  18.7   8. ]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  17.2   8. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 16.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  16.7]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  25.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 61.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  26.  -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  27.3 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 25.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   61.3]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   66.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 27.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  27.3 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  25.8 -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  25.8 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  21.6 -15. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  25.4]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  21.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 66.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  21.6 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  23.7 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 23.7]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   66.2]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   70.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 70.5]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   70.5]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   66.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 66.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  23.7 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  25.6 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 21.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   66.1]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   70.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 70.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  25.6 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  27.7 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 21.2]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   70.2]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   73.9]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 27.7]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  27.7 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  26.1 -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 26.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  26.1 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  22.6 -15. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 21.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  22.6 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  21.  -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  0\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 21.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  21.2]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  29.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 21.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  21.  -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  19.6 -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  19.6 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  16.  -15. ]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 16.]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  29.8]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  25.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  16.  -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  14.8 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 25.1]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  17.2   8. ]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  17.2  18.2]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  14.8 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  18.2 -15. ]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 73.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   73.9]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   81.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  25.1]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  21.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 18.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  17.2  18.2]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  17.2  14.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 81.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   81.4]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   76.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  21.6]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -2.2  17.8]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 18.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -2.2  17.8]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -1.5  17.8]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 17.2]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  18.2 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  14.8 -15. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 17.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  17.2  14.8]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  15.9  14.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 15.9]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -1.5  17.8]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -1.5  26.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 14.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  14.8 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  13.5 -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 26.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  13.5 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  11.3 -15. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 26.3]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  15.9  14.8]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  15.4  14.8]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 26.3]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -1.5  26.3]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -1.5  34.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 11.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   76.8]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   67. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 15.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  11.3 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  10.4 -15. ]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 15.4]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  15.4  14.8]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  12.1  14.8]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 14.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   67. ]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -3.   58.5]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 10.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -3.   58.5]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -2.9  58.5]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 14.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  10.4 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   7.5 -15. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  3\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 34.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   7.5 -15. ]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   7.5 -13.7]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 14.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   7.5 -13.7]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   6.9 -13.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 34.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  12.1  14.8]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  12.1  25.1]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 25.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  12.1  25.1]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  12.1  21.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 58.5]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   6.9 -13.7]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   9.9 -13.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 34.8]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -2.9  58.5]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -2.9  64.4]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 9.9]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -1.5  34.8]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -1.5  29.1]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 29.1]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   9.9 -13.7]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  10.2 -13.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  1\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 64.4]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -1.5  29.1]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -1.5  40.3]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  0\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 10.2]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -2.9  64.4]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -2.8  64.4]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 21.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  10.2 -13.7]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2   7.9 -13.7]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 21.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -1.5  40.3]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -1.5  35. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 64.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2   7.9 -13.7]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  11.3 -13.7]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 35.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -2.8  64.4]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -2.8  69.8]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 21.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  11.3 -13.7]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  11.3 -13.6]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 69.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  12.1  21.6]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  12.1  22. ]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 22.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  11.3 -13.6]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  10.9 -13.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 35.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  12.1  22. ]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  12.1  31.6]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  1\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 69.8]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  10.9 -13.6]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  14.4 -13.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  2\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 31.6]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -2.8  69.8]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -2.8  74.3]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  3\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 35.]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  14.4 -13.6]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  14.8 -13.6]]\n",
      "Real state update:  1  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  0\n",
      "Max Q:  [ 31.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -1.5  35. ]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -1.8  35. ]]\n",
      "Reset\n",
      "Observation:  2\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 74.3]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  12.1  31.6]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  12.1  31.4]]\n",
      "Reset\n",
      "Observation:  0\n",
      "Real state update:  0  -->  1\n",
      "\n",
      "Observation:  2\n",
      "Real state:  1\n",
      "Action number:  0\n",
      "Max Q:  [ 31.4]\n",
      "Reward:  -10\n",
      "Before update:  [[  0.5   0.2   0.2   0.2  14.8 -13.6]]\n",
      "After update:  [[  0.5   0.2   0.2   0.2  14.8 -13.6]]\n",
      "Real state update:  1  -->  3\n",
      "\n",
      "Observation:  3\n",
      "Real state:  3\n",
      "Action number:  1\n",
      "Max Q:  [ 35.]\n",
      "Reward:  90\n",
      "Before update:  [[  0.2   0.2   0.5   0.2  12.1  31.4]]\n",
      "After update:  [[  0.2   0.2   0.5   0.2  12.1  40.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 40.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -2.8  74.3]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -2.8  67.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 14.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -2.8  67.1]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -2.8  58.6]]\n",
      "Reset\n",
      "Observation:  3\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  2\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 40.1]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.2   0.2   0.5  -1.8  35. ]]\n",
      "After update:  [[  0.2   0.2   0.2   0.5  -1.8  31.7]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  1\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 58.6]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -2.8  58.6]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -2.8  54.4]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2\n",
      "\n",
      "Observation:  0\n",
      "Real state:  2\n",
      "Action number:  1\n",
      "Max Q:  [ 14.8]\n",
      "Reward:  -30\n",
      "Before update:  [[  0.2   0.5   0.2   0.2  -2.8  54.4]]\n",
      "After update:  [[  0.2   0.5   0.2   0.2  -2.8  47.1]]\n",
      "Reset\n",
      "Observation:  1\n",
      "Real state update:  0  -->  2"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "class Maze:\n",
    "    def __init__(self):\n",
    "\n",
    "        # array for storing reward values\n",
    "        self.r = np.array([[-1, -10, -30,  -1],   # State S0\n",
    "                           [-1,  -1, -30,  90],   # State S1\n",
    "                           [-1,  -1,  -1,  -1],   # State S2\n",
    "                           [-1,  -1,  -1,  -1]])  # State S3\n",
    "\n",
    "        # current reward\n",
    "        self.reward = -1\n",
    "\n",
    "        # keep track of total reward\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        # list of states\n",
    "        self.state_list = [0, 1, 2, 3]\n",
    "\n",
    "        # belief state, all same to start\n",
    "        self.belief_state = [0.3, 0.3, 0.3, 0.3]  # Choice to be made in s0 and s1\n",
    "\n",
    "        # set up belief table\n",
    "        self.belief_table = []\n",
    "\n",
    "        for s0 in range(0, 11):\n",
    "            for s1 in range (0, 11):\n",
    "                for s2 in range (0, 11):\n",
    "                    for s3 in range (0, 11):\n",
    "                        self.belief_table.append([s0, s1, s2, s3, 0, 0])\n",
    "\n",
    "        # convert to array\n",
    "        self.belief_table = np.vstack(self.belief_table)\n",
    "\n",
    "        # divide all by 10\n",
    "        self.belief_table = self.belief_table/10.0\n",
    "\n",
    "        # index of belief table corresponding to belief state\n",
    "        self.belief_table_index = -1\n",
    "\n",
    "        # index of belief table corresponding to previous belief state\n",
    "        self.previous_belief_table_index = -1\n",
    "\n",
    "        # keep track of real state\n",
    "        self.real_state = -1\n",
    "\n",
    "        # action number: 0 = Pull lever, 1 = Enter magazine\n",
    "        self.action_number = -1\n",
    "\n",
    "        # state to move to next\n",
    "        self.next_state = -1\n",
    "\n",
    "        # list to keep track of expected value\n",
    "        self.expected_value_array = []\n",
    "\n",
    "    def reset(self, initial_state):\n",
    "\n",
    "        print \"Reset\"\n",
    "\n",
    "        self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "        self.real_state = initial_state\n",
    "\n",
    "    def observe(self, observation_chance):\n",
    "\n",
    "        rand_num = random.random()\n",
    "        if rand_num < observation_chance:\n",
    "            return self.real_state\n",
    "        else:\n",
    "            random_state = random.choice(self.state_list)\n",
    "\n",
    "            while random_state == self.real_state:\n",
    "                random_state = random.choice(self.state_list)\n",
    "\n",
    "            return random_state\n",
    "\n",
    "    def update_belief_state(self, observation_state, observation_chance):\n",
    "\n",
    "        # Temporary list\n",
    "        temp_belief_state = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        # Use Bayes' Rule to update belief state\n",
    "        for n in range(len(temp_belief_state)):\n",
    "            if observation_state == n:\n",
    "                temp_belief_state[n] = observation_chance * self.belief_state[n]\n",
    "            else:\n",
    "                temp_belief_state[n] = ((1 - observation_chance)/(len(self.state_list) - 1)) * self.belief_state[n]\n",
    "\n",
    "        # Normalise so the belief state sums to 1\n",
    "        temp_total = sum(temp_belief_state)\n",
    "        self.belief_state = list(n/temp_total for n in temp_belief_state)\n",
    "\n",
    "        # Round to 1 decimal place\n",
    "        self.belief_state = [round(n, 1) for n in self.belief_state]\n",
    "\n",
    "    def choose_next_action(self, epsilon):\n",
    "\n",
    "        # get the q_values of the two actions\n",
    "        q_value_a0 = (self.belief_table[self.belief_table_index, 4])\n",
    "\n",
    "        q_value_a1 = (self.belief_table[self.belief_table_index, 5])\n",
    "\n",
    "        # choose an action using epsilon greedy\n",
    "        rand_num = random.random()\n",
    "\n",
    "        if q_value_a0 > q_value_a1:\n",
    "\n",
    "            if rand_num > epsilon:\n",
    "                self.action_number = 0  # Exploit\n",
    "\n",
    "            else:\n",
    "                self.action_number = 1  # Explore\n",
    "\n",
    "        elif q_value_a1 > q_value_a0:\n",
    "\n",
    "            if rand_num > epsilon:\n",
    "                self.action_number = 1  # Exploit\n",
    "\n",
    "            else:\n",
    "                self.action_number = 0  # Explore\n",
    "\n",
    "        else:\n",
    "            self.action_number = random.randint(0, 1)\n",
    "\n",
    "    # get corresponding next state from current state and action number\n",
    "    def get_next_state(self):\n",
    "\n",
    "        if self.real_state == 0:\n",
    "\n",
    "            if self.action_number == 0:\n",
    "\n",
    "                self.next_state = 1\n",
    "\n",
    "            elif self.action_number == 1:\n",
    "\n",
    "                self.next_state = 2\n",
    "\n",
    "        elif self.real_state == 1:\n",
    "\n",
    "            if self.action_number == 0:\n",
    "\n",
    "                self.next_state = 2\n",
    "\n",
    "            elif self.action_number == 1:\n",
    "\n",
    "                self.next_state = 3\n",
    "\n",
    "    def get_belief_table_index(self):\n",
    "\n",
    "        self.belief_table_index = np.where((self.belief_table[:, 0] == self.belief_state[0])\n",
    "                                           & (self.belief_table[:, 1] == self.belief_state[1])\n",
    "                                           & (self.belief_table[:, 2] == self.belief_state[2])\n",
    "                                           & (self.belief_table[:, 3] == self.belief_state[3]))[0]\n",
    "\n",
    "    # get highest Q-value in belief state\n",
    "    def calculate_max_q_value(self):\n",
    "\n",
    "        # get the Q-values of the two actions\n",
    "        q_value_a0 = self.belief_table[self.belief_table_index, 4]\n",
    "\n",
    "        q_value_a1 = self.belief_table[self.belief_table_index, 5]\n",
    "\n",
    "        # return the highest\n",
    "        if q_value_a0 > q_value_a1:\n",
    "            return q_value_a0\n",
    "        else:\n",
    "            return q_value_a1\n",
    "\n",
    "    def update_belief_table(self, alpha, gamma):\n",
    "\n",
    "        # get correct element index for the select action\n",
    "        if self.action_number == 0:\n",
    "            location = 4\n",
    "        else:\n",
    "            location = 5\n",
    "\n",
    "        max_q_value = self.calculate_max_q_value()\n",
    "\n",
    "        previous_q = self.belief_table[self.previous_belief_table_index, location]\n",
    "\n",
    "        print \"Real state: \", self.real_state\n",
    "        print \"Action number: \", self.action_number\n",
    "        print \"Max Q: \", max_q_value\n",
    "        print \"Reward: \", self.reward\n",
    "\n",
    "        print \"Before update: \", self.belief_table[self.previous_belief_table_index]\n",
    "\n",
    "        # update Q-value\n",
    "        self.belief_table[self.previous_belief_table_index, location] = previous_q + alpha * (self.reward + gamma * max_q_value - previous_q)\n",
    "\n",
    "        # round to 1 decimal place\n",
    "        self.belief_table[self.previous_belief_table_index, location] = round(self.belief_table[self.previous_belief_table_index, location], 1)\n",
    "\n",
    "        print \"After update: \", self.belief_table[self.previous_belief_table_index]\n",
    "\n",
    "    def run_experiment(self, initial_state, observation_chance, alpha, gamma, epsilon, num_runs_wanted):\n",
    "\n",
    "        # set current state to initial state wanted\n",
    "        self.real_state = initial_state\n",
    "\n",
    "        # make an initial run\n",
    "\n",
    "        # make an observation\n",
    "        observation = self.observe(observation_chance)\n",
    "\n",
    "        # update belief state with observation\n",
    "        self.update_belief_state(observation, observation_chance)\n",
    "\n",
    "        # get the corresponding index in the belief table\n",
    "        self.get_belief_table_index()\n",
    "\n",
    "        # choose next action\n",
    "        self.choose_next_action(epsilon)\n",
    "\n",
    "        # get the next state\n",
    "        self.get_next_state()\n",
    "\n",
    "        # get the reward for moving from current state to next state\n",
    "        self.reward = self.r[self.real_state][self.next_state]\n",
    "\n",
    "        # update total reward\n",
    "        self.total_reward += self.reward\n",
    "\n",
    "        # move to the next state\n",
    "        self.real_state = self.next_state\n",
    "        self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "        print \"Real state update: \", self.real_state, \" --> \", self.next_state\n",
    "\n",
    "        # variables to keep track\n",
    "        num_rewards_received = 0\n",
    "        num_fails = 0\n",
    "\n",
    "        num_runs = 0\n",
    "\n",
    "        while num_runs < num_runs_wanted:\n",
    "\n",
    "            print \"\"\n",
    "\n",
    "            # make an observation\n",
    "            observation = self.observe(observation_chance)\n",
    "\n",
    "            # update belief state with observation\n",
    "            self.update_belief_state(observation, observation_chance)\n",
    "\n",
    "            # keep track of the previous belief_table_index\n",
    "            self.previous_belief_table_index = self.belief_table_index\n",
    "\n",
    "            # get the corresponding index in the belief table\n",
    "            self.get_belief_table_index()\n",
    "\n",
    "            print \"Observation: \", observation\n",
    "\n",
    "            # update the belief table using Q-learning\n",
    "            self.update_belief_table(alpha, gamma)\n",
    "\n",
    "            # if terminal state reached\n",
    "            if self.real_state == 2 or self.real_state == 3:\n",
    "\n",
    "                if self.real_state == 2:\n",
    "                    num_fails += 1\n",
    "                elif self.real_state == 3:\n",
    "                    num_rewards_received += 1\n",
    "\n",
    "                num_runs += 1\n",
    "\n",
    "                self.expected_value_array = np.append(self.expected_value_array, self.total_reward / (80 * num_runs))\n",
    "\n",
    "                # reset to inital values\n",
    "                self.reset(initial_state)\n",
    "\n",
    "                # make an observation\n",
    "                observation = self.observe(observation_chance)\n",
    "\n",
    "                # update belief state with observation\n",
    "                self.update_belief_state(observation, observation_chance)\n",
    "\n",
    "                print \"Observation: \", observation\n",
    "\n",
    "                # get the corresponding index in the belief table\n",
    "                self.get_belief_table_index()\n",
    "\n",
    "                # keep track of the previous belief_table_index\n",
    "                self.previous_belief_table_index = self.belief_table_index\n",
    "\n",
    "            # choose next action\n",
    "            self.choose_next_action(epsilon)\n",
    "\n",
    "            # get the next state\n",
    "            self.get_next_state()\n",
    "\n",
    "            # get the reward for moving from current state to next state\n",
    "            self.reward = self.r[self.real_state][self.next_state]\n",
    "\n",
    "            # update total reward\n",
    "            self.total_reward += self.reward\n",
    "\n",
    "            print \"Real state update: \", self.real_state, \" --> \", self.next_state\n",
    "\n",
    "            # move to the next state\n",
    "            self.real_state = self.next_state\n",
    "            self.belief_state = [0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "        print \"\"\n",
    "        print \"Number of fails:\"\n",
    "        print num_fails\n",
    "\n",
    "        print \"\"\n",
    "        print \"Number of rewards:\"\n",
    "        print num_rewards_received\n",
    "\n",
    "experiment_instance = Maze()\n",
    "experiment_instance.run_experiment(0, 1.0, 0.1, 0.8, 0.1, 500)\n",
    "expected_value_1p00 = experiment_instance.expected_value_array\n",
    "\n",
    "experiment_instance = Maze()\n",
    "experiment_instance.run_experiment(0, 0.75, 0.1, 0.8, 0.1, 500)\n",
    "expected_value_0p75 = experiment_instance.expected_value_array\n",
    "\n",
    "experiment_instance = Maze()\n",
    "experiment_instance.run_experiment(0, 0.50, 0.1, 0.8, 0.1, 500)\n",
    "expected_value_0p50 = experiment_instance.expected_value_array\n",
    "\n",
    "experiment_instance = Maze()\n",
    "experiment_instance.run_experiment(0, 0.25, 0.1, 0.8, 0.1, 500)\n",
    "expected_value_0p25 = experiment_instance.expected_value_array\n",
    "\n",
    "\n",
    "plt.plot(expected_value_1p00, label=\"Obs Val 1.00\")  # plot the results, and give a label\n",
    "plt.plot(expected_value_0p75, label=\"Obs Val 0.75\")\n",
    "plt.plot(expected_value_0p50, label=\"Obs Val 0.50\")\n",
    "plt.plot(expected_value_0p25, label=\"Obs Val 0.25\")\n",
    "plt.ylim(-0.5, 1)  # limits for the y-axis\n",
    "plt.legend(loc='best')  # display the graph key\n",
    "plt.savefig('expected_value_graph.png')  # save graph to folder\n",
    "plt.show()  # display the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    experiment_instance = Maze()\n",
    "    experiment_instance.run_experiment(0, 1.00, 0.1, 0.8, 0.1, 500)\n",
    "    expected_value_1p00 = experiment_instance.expected_value_array\n",
    "\n",
    "    experiment_instance = Maze()\n",
    "    experiment_instance.run_experiment(0, 0.75, 0.1, 0.8, 0.1, 500)\n",
    "    expected_value_0p75 = experiment_instance.expected_value_array\n",
    "\n",
    "    experiment_instance = Maze()\n",
    "    experiment_instance.run_experiment(0, 0.50, 0.1, 0.8, 0.1, 500)\n",
    "    expected_value_0p50 = experiment_instance.expected_value_array\n",
    "\n",
    "    experiment_instance = Maze()\n",
    "    experiment_instance.run_experiment(0, 0.25, 0.1, 0.8, 0.1, 500)\n",
    "    expected_value_0p25 = experiment_instance.expected_value_array\n",
    "    \n",
    "We generate and run instances of the experiment for observation values of 1.00, 0.75, 0.50, 0.25. We then assign the expected_value_array from each of the experiments to variables.\n",
    "\n",
    "1p00 reads 'one point zero zero' (1.0), 0p75 reads 'zero point seven five' (0.75), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    plt.plot(expected_value_1p00, label=\"EV 1.00\")  # plot the results, and give a label\n",
    "    plt.plot(expected_value_0p75, label=\"EV 0.75\")\n",
    "    plt.plot(expected_value_0p50, label=\"EV 0.50\")\n",
    "    plt.plot(expected_value_0p25, label=\"EV 0.25\")\n",
    "    plt.ylim(-0.5, 1)  # limits for the y-axis\n",
    "    plt.legend(loc='best')  # display the graph key\n",
    "    plt.savefig('expected_value_graph.png')  # save graph to folder\n",
    "    plt.show()  # display the graph\n",
    "    \n",
    "We plot a graph with the expected values of each of the observation values. The code for plotting graphs is similar to the other two tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code, the graph will be saved to the same folder as the notebook as 'expected_value_graph.png'. You should have a graph similar to this:\n",
    "\n",
    "<img src=\"expected_value.png\" width=\"800\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the higher the observation value, the higher the expected value, as we would expect. \n",
    "\n",
    "Congratulations! You've now completed this tutorial series. It is hoped that the series has helped to provide an insight into using computational modelling techniques within cognition, and that you will be able to create your own models in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "Brown University. (1999), ‘Background on Solving POMDPs’, http://cs.brown.edu/research/ai/pomdp/tutorial/pomdp-solving.html. [Online; accessed 02 February 2017].\n",
    "\n",
    "Daw, N. D., Niv, Y. and Dayan, P. (2005), ‘Uncertainty-based competition between pre- frontal and dorsolateral striatal systems for behavioral control’, Nature neuroscience 8(12), 1704–1711.\n",
    "\n",
    "Sutton, R. S. and Barto, A. G. (1998), Reinforcement learning: An introduction, Vol. 1, MIT press Cambridge."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
